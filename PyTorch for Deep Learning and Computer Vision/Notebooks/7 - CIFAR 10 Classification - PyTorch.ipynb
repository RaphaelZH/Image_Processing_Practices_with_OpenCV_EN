{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "aaaf9137",
   "metadata": {},
   "source": [
    "# Importing Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "444b4926",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-15T21:28:27.317416Z",
     "start_time": "2023-01-15T21:28:26.180732Z"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "#from torch import nn\n",
    "#import torch.nn.functional as F\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "#import numpy as np\n",
    "\n",
    "#import requests\n",
    "\n",
    "import matplotlib as mpl\n",
    "#import matplotlib.gridspec as gridspec\n",
    "import matplotlib.pyplot as plt\n",
    "#from mpl_toolkits.axes_grid1.inset_locator import inset_axes\n",
    "\n",
    "#from PIL import Image, ImageOps\n",
    "\n",
    "#import reprlib\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9e65d2d6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-15T21:28:27.323339Z",
     "start_time": "2023-01-15T21:28:27.318699Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 4, 4])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.tensor([[[1, 2, 3, 4], [1, 2, 3, 4], [1, 2, 3, 4], [1, 2, 3, 4]],\n",
    "                  [[1, 2, 3, 4], [1, 2, 3, 4], [1, 2, 3, 4], [1, 2, 3, 4]]])\n",
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7cb01992",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-15T21:28:27.326741Z",
     "start_time": "2023-01-15T21:28:27.324322Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 1, 2, 4, 4])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.unsqueeze(0).unsqueeze(1).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f62ce0f",
   "metadata": {},
   "source": [
    "# Configuring Visualization Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "98e4893c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-15T21:28:27.333999Z",
     "start_time": "2023-01-15T21:28:27.328069Z"
    }
   },
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d4c30a7d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-15T21:28:27.336703Z",
     "start_time": "2023-01-15T21:28:27.334977Z"
    }
   },
   "outputs": [],
   "source": [
    "XINHUI = \"#7a7374\"\n",
    "XUEBAI = \"#fffef9\"\n",
    "YINBAI = \"#f1f0ed\"\n",
    "YINHUI = \"#918072\"\n",
    "\n",
    "figure_size = (16, 9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c96adaa6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-15T21:28:27.340107Z",
     "start_time": "2023-01-15T21:28:27.337837Z"
    }
   },
   "outputs": [],
   "source": [
    "custom_params = {\n",
    "    \"axes.axisbelow\": True,\n",
    "    \"axes.edgecolor\": YINBAI,\n",
    "    \"axes.facecolor\": XUEBAI,\n",
    "    \"axes.grid\": True,\n",
    "    \"axes.labelcolor\": XINHUI,\n",
    "    \"axes.spines.right\": False,\n",
    "    \"axes.spines.top\": False,\n",
    "    \"axes.titlecolor\": XINHUI,\n",
    "    \"figure.edgecolor\": YINBAI,\n",
    "    \"figure.facecolor\": XUEBAI,\n",
    "    \"grid.alpha\": .8,\n",
    "    \"grid.color\": YINBAI,\n",
    "    \"grid.linestyle\": \"--\",\n",
    "    \"grid.linewidth\": 1.2,\n",
    "    \"legend.edgecolor\": YINHUI,\n",
    "    \"patch.edgecolor\": XUEBAI,\n",
    "    \"patch.force_edgecolor\": True,\n",
    "    \"text.color\": XINHUI,\n",
    "    \"xtick.color\": YINHUI,\n",
    "    \"ytick.color\": YINHUI,\n",
    "}\n",
    "\n",
    "mpl.rcParams.update(custom_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fad658d",
   "metadata": {},
   "source": [
    "# Configuring Other Notebook Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8e37712a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-15T21:28:27.342450Z",
     "start_time": "2023-01-15T21:28:27.340940Z"
    }
   },
   "outputs": [],
   "source": [
    "#reprlib_rules = reprlib.Repr()\n",
    "#reprlib_rules.maxother = 250"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83421c04",
   "metadata": {},
   "source": [
    "# Pre-installing Custom Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "22b163a3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-15T21:28:27.344804Z",
     "start_time": "2023-01-15T21:28:27.343223Z"
    }
   },
   "outputs": [],
   "source": [
    "sys.path.append(\"../\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5806d1bf",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-15T21:28:27.508302Z",
     "start_time": "2023-01-15T21:28:27.345961Z"
    }
   },
   "outputs": [],
   "source": [
    "from Modules import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ca3ace39",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-15T21:28:27.513733Z",
     "start_time": "2023-01-15T21:28:27.510986Z"
    }
   },
   "outputs": [],
   "source": [
    "def im_convert(tensor):\n",
    "    image = tensor.cpu().clone().detach().numpy()\n",
    "    image = image.transpose(1, 2, 0)\n",
    "    image = image * np.array((0.5, 0.5, 0.5)) + np.array((0.5, 0.5, 0.5))\n",
    "    image = image.clip(0, 1)\n",
    "    return image\n",
    "\n",
    "\n",
    "def image_to_scalar_data(tensor):\n",
    "    image = tensor.cpu().clone().detach().numpy()\n",
    "    image = image.transpose(1, 2, 0)\n",
    "    image = image.reshape(image.shape[0], -1)\n",
    "    image = image * 0.5 + 0.5\n",
    "    image = image.clip(0, 1)\n",
    "    return image"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e375a284",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-12T17:56:18.222325Z",
     "start_time": "2023-01-12T17:56:18.218703Z"
    }
   },
   "source": [
    "# Practicing in Stages\n",
    "\n",
    "## Testing LeNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ef1e965b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-15T21:28:35.878958Z",
     "start_time": "2023-01-15T21:28:27.514876Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "\u001b[4m\u001b[32mLoading and transformation of the CIFAR10 training and validation datasets\u001b[0m\n",
      "\n",
      "\u001b[1m\u001b[32m    +-------------------------------------------------------------+\u001b[0m\n",
      "\u001b[1m\u001b[32m    | Statement                                                   |\u001b[0m\n",
      "\u001b[1m\u001b[32m    +-------------------------------------------------------------+\u001b[0m\n",
      "\u001b[1m\u001b[32m    | transform = transforms.Compose([                            |\u001b[0m\n",
      "\u001b[1m\u001b[32m    |     transforms.Resize((32, 32)),                            |\u001b[0m\n",
      "\u001b[1m\u001b[32m    |     transforms.ToTensor(),                                  |\u001b[0m\n",
      "\u001b[1m\u001b[32m    |     transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))  |\u001b[0m\n",
      "\u001b[1m\u001b[32m    | ])                                                          |\u001b[0m\n",
      "\u001b[1m\u001b[32m    |                                                             |\u001b[0m\n",
      "\u001b[1m\u001b[32m    | training_dataset = datasets.CIFAR10(root=\"../Datasets\",     |\u001b[0m\n",
      "\u001b[1m\u001b[32m    |                                     train=True,             |\u001b[0m\n",
      "\u001b[1m\u001b[32m    |                                     download=True,          |\u001b[0m\n",
      "\u001b[1m\u001b[32m    |                                     transform=transform)    |\u001b[0m\n",
      "\u001b[1m\u001b[32m    |                                                             |\u001b[0m\n",
      "\u001b[1m\u001b[32m    | validation_dataset = datasets.CIFAR10(root=\"../Datasets\",   |\u001b[0m\n",
      "\u001b[1m\u001b[32m    |                                       train=False,          |\u001b[0m\n",
      "\u001b[1m\u001b[32m    |                                       download=True,        |\u001b[0m\n",
      "\u001b[1m\u001b[32m    |                                       transform=transform)  |\u001b[0m\n",
      "\u001b[1m\u001b[32m    |                                                             |\u001b[0m\n",
      "\u001b[1m\u001b[32m    | training_loader =                                           |\u001b[0m\n",
      "\u001b[1m\u001b[32m    |     torch.utils.data.DataLoader(training_dataset,           |\u001b[0m\n",
      "\u001b[1m\u001b[32m    |                                                             |\u001b[0m\n",
      "\u001b[1m\u001b[32m    |     batch_size=100,                                         |\u001b[0m\n",
      "\u001b[1m\u001b[32m    |                                               shuffle=True) |\u001b[0m\n",
      "\u001b[1m\u001b[32m    |                                                             |\u001b[0m\n",
      "\u001b[1m\u001b[32m    | validation_loader =                                         |\u001b[0m\n",
      "\u001b[1m\u001b[32m    |     torch.utils.data.DataLoader(validation_dataset,         |\u001b[0m\n",
      "\u001b[1m\u001b[32m    |                                                             |\u001b[0m\n",
      "\u001b[1m\u001b[32m    |     batch_size=100,                                         |\u001b[0m\n",
      "\u001b[1m\u001b[32m    |                                                             |\u001b[0m\n",
      "\u001b[1m\u001b[32m    |     shuffle=False)                                          |\u001b[0m\n",
      "\u001b[1m\u001b[32m    +-------------------------------------------------------------+\u001b[0m\n",
      "\u001b[1m\u001b[32m    +--------------------+----------------------------------------+\u001b[0m\n",
      "\u001b[1m\u001b[32m    | Variable           | Value                                  |\u001b[0m\n",
      "\u001b[1m\u001b[32m    +--------------------+----------------------------------------+\u001b[0m\n",
      "\u001b[1m\u001b[32m    | transform          | Compose(                               |\u001b[0m\n",
      "\u001b[1m\u001b[32m    |                    |     Resize(size=(32, 32),              |\u001b[0m\n",
      "\u001b[1m\u001b[32m    |                    |         interpolation=bilinear,        |\u001b[0m\n",
      "\u001b[1m\u001b[32m    |                    |         max_size=None, antialias=None) |\u001b[0m\n",
      "\u001b[1m\u001b[32m    |                    |     ToTensor()                         |\u001b[0m\n",
      "\u001b[1m\u001b[32m    |                    |     Normalize(mean=(0.5, 0.5, 0.5),    |\u001b[0m\n",
      "\u001b[1m\u001b[32m    |                    |         std=(0.5, 0.5, 0.5))           |\u001b[0m\n",
      "\u001b[1m\u001b[32m    |                    | )                                      |\u001b[0m\n",
      "\u001b[1m\u001b[32m    | training_dataset   | Dataset CIFAR10                        |\u001b[0m\n",
      "\u001b[1m\u001b[32m    |                    |     Number of datapoints: 50000        |\u001b[0m\n",
      "\u001b[1m\u001b[32m    |                    |     Root location: ../Datasets         |\u001b[0m\n",
      "\u001b[1m\u001b[32m    |                    |     Split: Train                       |\u001b[0m\n",
      "\u001b[1m\u001b[32m    |                    |     StandardTransform                  |\u001b[0m\n",
      "\u001b[1m\u001b[32m    |                    | Transform: Compose(                    |\u001b[0m\n",
      "\u001b[1m\u001b[32m    |                    |                Resize(size=(32, 32),   |\u001b[0m\n",
      "\u001b[1m\u001b[32m    |                    |         interpolation=bilinear,        |\u001b[0m\n",
      "\u001b[1m\u001b[32m    |                    |         max_size=None, antialias=None) |\u001b[0m\n",
      "\u001b[1m\u001b[32m    |                    |                ToTensor()              |\u001b[0m\n",
      "\u001b[1m\u001b[32m    |                    |                Normalize(mean=(0.5,    |\u001b[0m\n",
      "\u001b[1m\u001b[32m    |                    |         0.5, 0.5), std=(0.5, 0.5,      |\u001b[0m\n",
      "\u001b[1m\u001b[32m    |                    |         0.5))                          |\u001b[0m\n",
      "\u001b[1m\u001b[32m    |                    |            )                           |\u001b[0m\n",
      "\u001b[1m\u001b[32m    | validation_dataset | Dataset CIFAR10                        |\u001b[0m\n",
      "\u001b[1m\u001b[32m    |                    |     Number of datapoints: 10000        |\u001b[0m\n",
      "\u001b[1m\u001b[32m    |                    |     Root location: ../Datasets         |\u001b[0m\n",
      "\u001b[1m\u001b[32m    |                    |     Split: Test                        |\u001b[0m\n",
      "\u001b[1m\u001b[32m    |                    |     StandardTransform                  |\u001b[0m\n",
      "\u001b[1m\u001b[32m    |                    | Transform: Compose(                    |\u001b[0m\n",
      "\u001b[1m\u001b[32m    |                    |                Resize(size=(32, 32),   |\u001b[0m\n",
      "\u001b[1m\u001b[32m    |                    |         interpolation=bilinear,        |\u001b[0m\n",
      "\u001b[1m\u001b[32m    |                    |         max_size=None, antialias=None) |\u001b[0m\n",
      "\u001b[1m\u001b[32m    |                    |                ToTensor()              |\u001b[0m\n",
      "\u001b[1m\u001b[32m    |                    |                Normalize(mean=(0.5,    |\u001b[0m\n",
      "\u001b[1m\u001b[32m    |                    |         0.5, 0.5), std=(0.5, 0.5,      |\u001b[0m\n",
      "\u001b[1m\u001b[32m    |                    |         0.5))                          |\u001b[0m\n",
      "\u001b[1m\u001b[32m    |                    |            )                           |\u001b[0m\n",
      "\u001b[1m\u001b[32m    +--------------------+----------------------------------------+\u001b[0m\n",
      "\u001b[1m\u001b[32m    +----------------------------------------+--------------------+\u001b[0m\n",
      "\u001b[1m\u001b[32m    | Expression                             | Result             |\u001b[0m\n",
      "\u001b[1m\u001b[32m    +----------------------------------------+--------------------+\u001b[0m\n",
      "\u001b[1m\u001b[32m    | len(training_loader)                   | 500                |\u001b[0m\n",
      "\u001b[1m\u001b[32m    | len(training_loader.dataset)           | 50000              |\u001b[0m\n",
      "\u001b[1m\u001b[32m    | list(training_loader)[0][0].shape      | torch.Size([100,   |\u001b[0m\n",
      "\u001b[1m\u001b[32m    |                                        |             3, 32, |\u001b[0m\n",
      "\u001b[1m\u001b[32m    |                                        |             32])   |\u001b[0m\n",
      "\u001b[1m\u001b[32m    | list(training_loader)[0][1].shape      | torch.Size([100])  |\u001b[0m\n",
      "\u001b[1m\u001b[32m    | next(iter(training_loader))[0].shape   | torch.Size([100,   |\u001b[0m\n",
      "\u001b[1m\u001b[32m    |                                        |             3, 32, |\u001b[0m\n",
      "\u001b[1m\u001b[32m    |                                        |             32])   |\u001b[0m\n",
      "\u001b[1m\u001b[32m    | next(iter(training_loader))[1].shape   | torch.Size([100])  |\u001b[0m\n",
      "\u001b[1m\u001b[32m    | len(validation_loader)                 | 100                |\u001b[0m\n",
      "\u001b[1m\u001b[32m    | len(validation_loader.dataset)         | 10000              |\u001b[0m\n",
      "\u001b[1m\u001b[32m    | list(validation_loader)[0][0].shape    | torch.Size([100,   |\u001b[0m\n",
      "\u001b[1m\u001b[32m    |                                        |             3, 32, |\u001b[0m\n",
      "\u001b[1m\u001b[32m    |                                        |             32])   |\u001b[0m\n",
      "\u001b[1m\u001b[32m    | list(validation_loader)[0][1].shape    | torch.Size([100])  |\u001b[0m\n",
      "\u001b[1m\u001b[32m    | next(iter(validation_loader))[0].shape | torch.Size([100,   |\u001b[0m\n",
      "\u001b[1m\u001b[32m    |                                        |             3, 32, |\u001b[0m\n",
      "\u001b[1m\u001b[32m    |                                        |             32])   |\u001b[0m\n",
      "\u001b[1m\u001b[32m    | next(iter(validation_loader))[1].shape | torch.Size([100])  |\u001b[0m\n",
      "\u001b[1m\u001b[32m    +----------------------------------------+--------------------+\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "transform = transforms.Compose([\n",
    "    transforms.Resize((32, 32)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "])\n",
    "\n",
    "training_dataset = datasets.CIFAR10(root=\"../Datasets\",\n",
    "                                    train=True,\n",
    "                                    download=True,\n",
    "                                    transform=transform)\n",
    "\n",
    "validation_dataset = datasets.CIFAR10(root=\"../Datasets\",\n",
    "                                      train=False,\n",
    "                                      download=True,\n",
    "                                      transform=transform)\n",
    "\n",
    "training_loader = torch.utils.data.DataLoader(training_dataset,\n",
    "                                              batch_size=100,\n",
    "                                              shuffle=True)\n",
    "\n",
    "validation_loader = torch.utils.data.DataLoader(validation_dataset,\n",
    "                                                batch_size=100,\n",
    "                                                shuffle=False)\n",
    "\n",
    "tabulation = Form_Generator()\n",
    "tabulation.heading_printer(\n",
    "    \"Loading and transformation of the CIFAR10 training and validation datasets\"\n",
    ")\n",
    "\n",
    "statements = [\n",
    "    \"\"\"\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((32, 32)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "])\n",
    "\n",
    "training_dataset = datasets.CIFAR10(root=\"../Datasets\",\n",
    "                                    train=True,\n",
    "                                    download=True,\n",
    "                                    transform=transform)\n",
    "\n",
    "validation_dataset = datasets.CIFAR10(root=\"../Datasets\",\n",
    "                                      train=False,\n",
    "                                      download=True,\n",
    "                                      transform=transform)\n",
    "\n",
    "training_loader = torch.utils.data.DataLoader(training_dataset,\n",
    "                                              batch_size=100,\n",
    "                                              shuffle=True)\n",
    "\n",
    "validation_loader = torch.utils.data.DataLoader(validation_dataset,\n",
    "                                                batch_size=100,\n",
    "                                                shuffle=False)\n",
    "\"\"\"\n",
    "]\n",
    "tabulation.statement_generator(statements)\n",
    "\n",
    "variables = [\n",
    "    \"transform\",\n",
    "    \"training_dataset\",\n",
    "    \"validation_dataset\",\n",
    "]\n",
    "values = [\n",
    "    str(transform),\n",
    "    str(training_dataset),\n",
    "    str(validation_dataset),\n",
    "]\n",
    "tabulation.variable_generator(variables, values)\n",
    "\n",
    "expressions = [\n",
    "    \"len(training_loader)\", \"len(training_loader.dataset)\",\n",
    "    \"list(training_loader)[0][0].shape\", \"list(training_loader)[0][1].shape\",\n",
    "    \"next(iter(training_loader))[0].shape\",\n",
    "    \"next(iter(training_loader))[1].shape\", \"len(validation_loader)\",\n",
    "    \"len(validation_loader.dataset)\", \"list(validation_loader)[0][0].shape\",\n",
    "    \"list(validation_loader)[0][1].shape\",\n",
    "    \"next(iter(validation_loader))[0].shape\",\n",
    "    \"next(iter(validation_loader))[1].shape\"\n",
    "]\n",
    "results = [\n",
    "    str(len(training_loader)),\n",
    "    str(len(training_loader.dataset)),\n",
    "    str(list(training_loader)[0][0].shape),\n",
    "    str(list(training_loader)[0][1].shape),\n",
    "    str(next(iter(training_loader))[0].shape),\n",
    "    str(next(iter(training_loader))[1].shape),\n",
    "    str(len(validation_loader)),\n",
    "    str(len(validation_loader.dataset)),\n",
    "    str(list(validation_loader)[0][0].shape),\n",
    "    str(list(validation_loader)[0][1].shape),\n",
    "    str(next(iter(validation_loader))[0].shape),\n",
    "    str(next(iter(validation_loader))[1].shape),\n",
    "]\n",
    "tabulation.expression_generator(expressions, results, 12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7c66c35c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-15T21:28:35.882462Z",
     "start_time": "2023-01-15T21:28:35.880030Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "500"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(training_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9b1d8c02",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-15T21:28:35.885907Z",
     "start_time": "2023-01-15T21:28:35.883597Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset CIFAR10\n",
       "    Number of datapoints: 50000\n",
       "    Root location: ../Datasets\n",
       "    Split: Train\n",
       "    StandardTransform\n",
       "Transform: Compose(\n",
       "               Resize(size=(32, 32), interpolation=bilinear, max_size=None, antialias=None)\n",
       "               ToTensor()\n",
       "               Normalize(mean=(0.5, 0.5, 0.5), std=(0.5, 0.5, 0.5))\n",
       "           )"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "557ade17",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  },
  "nbTranslate": {
   "displayLangs": [
    "*"
   ],
   "hotkey": "alt-t",
   "langInMainMenu": true,
   "sourceLang": "en",
   "targetLang": "fr",
   "useGoogleTranslate": true
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "345.594px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
