{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "aaaf9137",
   "metadata": {},
   "source": [
    "# Importing Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "444b4926",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-20T22:45:57.170877Z",
     "start_time": "2023-01-20T22:45:56.167559Z"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "#from torch import nn\n",
    "#import torch.nn.functional as F\n",
    "from torchvision import datasets, models, transforms\n",
    "\n",
    "#import numpy as np\n",
    "\n",
    "#import requests\n",
    "\n",
    "import matplotlib as mpl\n",
    "#import matplotlib.gridspec as gridspec\n",
    "import matplotlib.pyplot as plt\n",
    "#from mpl_toolkits.axes_grid1.inset_locator import inset_axes\n",
    "\n",
    "#from PIL import Image, ImageOps\n",
    "\n",
    "#import reprlib\n",
    "from git.repo.base import Repo\n",
    "import sys"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f62ce0f",
   "metadata": {},
   "source": [
    "# Configuring Visualization Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "98e4893c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-20T22:45:57.180027Z",
     "start_time": "2023-01-20T22:45:57.172552Z"
    }
   },
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d4c30a7d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-20T22:45:57.182570Z",
     "start_time": "2023-01-20T22:45:57.180879Z"
    }
   },
   "outputs": [],
   "source": [
    "XINHUI = \"#7a7374\"\n",
    "XUEBAI = \"#fffef9\"\n",
    "YINBAI = \"#f1f0ed\"\n",
    "YINHUI = \"#918072\"\n",
    "\n",
    "figure_size = (16, 9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c96adaa6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-20T22:45:57.185474Z",
     "start_time": "2023-01-20T22:45:57.183517Z"
    }
   },
   "outputs": [],
   "source": [
    "custom_params = {\n",
    "    \"axes.axisbelow\": True,\n",
    "    \"axes.edgecolor\": YINBAI,\n",
    "    \"axes.facecolor\": XUEBAI,\n",
    "    \"axes.grid\": True,\n",
    "    \"axes.labelcolor\": XINHUI,\n",
    "    \"axes.spines.right\": False,\n",
    "    \"axes.spines.top\": False,\n",
    "    \"axes.titlecolor\": XINHUI,\n",
    "    \"figure.edgecolor\": YINBAI,\n",
    "    \"figure.facecolor\": XUEBAI,\n",
    "    \"grid.alpha\": .8,\n",
    "    \"grid.color\": YINBAI,\n",
    "    \"grid.linestyle\": \"--\",\n",
    "    \"grid.linewidth\": 1.2,\n",
    "    \"legend.edgecolor\": YINHUI,\n",
    "    \"patch.edgecolor\": XUEBAI,\n",
    "    \"patch.force_edgecolor\": True,\n",
    "    \"text.color\": XINHUI,\n",
    "    \"xtick.color\": YINHUI,\n",
    "    \"ytick.color\": YINHUI,\n",
    "}\n",
    "\n",
    "mpl.rcParams.update(custom_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9afdc66d",
   "metadata": {},
   "source": [
    "# Configuring Pre-configured Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "113df146",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-20T22:45:57.195528Z",
     "start_time": "2023-01-20T22:45:57.186308Z"
    }
   },
   "outputs": [],
   "source": [
    "device = torch.device(\"mps:0\" if torch.backends.mps.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fad658d",
   "metadata": {},
   "source": [
    "# Configuring Other Notebook Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8e37712a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-20T22:45:57.198713Z",
     "start_time": "2023-01-20T22:45:57.197327Z"
    }
   },
   "outputs": [],
   "source": [
    "#reprlib_rules = reprlib.Repr()\n",
    "#reprlib_rules.maxother = 250"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83421c04",
   "metadata": {},
   "source": [
    "# Pre-installing Custom Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "22b163a3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-20T22:45:57.200752Z",
     "start_time": "2023-01-20T22:45:57.199422Z"
    }
   },
   "outputs": [],
   "source": [
    "sys.path.append(\"../\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5806d1bf",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-20T22:45:57.344612Z",
     "start_time": "2023-01-20T22:45:57.201657Z"
    }
   },
   "outputs": [],
   "source": [
    "from Modules import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ca3ace39",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-20T22:45:57.346769Z",
     "start_time": "2023-01-20T22:45:57.345421Z"
    }
   },
   "outputs": [],
   "source": [
    "#def im_convert(tensor):\n",
    "#    image = tensor.cpu().clone().detach().numpy()\n",
    "#    image = image.transpose(1, 2, 0)\n",
    "#    image = image * np.array((0.5, 0.5, 0.5)) + np.array((0.5, 0.5, 0.5))\n",
    "#    image = image.clip(0, 1)\n",
    "#    return image"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a05a16e9",
   "metadata": {},
   "source": [
    "# Loading Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "13f52d64",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-20T22:45:57.365730Z",
     "start_time": "2023-01-20T22:45:57.347435Z"
    }
   },
   "outputs": [],
   "source": [
    "data_path = \"../Datasets/ants_and_bees/\"\n",
    "\n",
    "try:\n",
    "    Repo.clone_from(\"https://github.com/jaddoescad/ants_and_bees\", data_path)\n",
    "except:\n",
    "    NameError"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e375a284",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-12T17:56:18.222325Z",
     "start_time": "2023-01-12T17:56:18.218703Z"
    }
   },
   "source": [
    "# Practicing in Stages\n",
    "\n",
    "## AlexNet and VGG16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fb972539",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-20T22:46:01.078546Z",
     "start_time": "2023-01-20T22:45:57.367029Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[4m\u001b[36mLoading and transformation of local training and validation datasets\u001b[0m\n",
      "\n",
      "\u001b[1m\u001b[36m    +-------------------------------------------------------------+\u001b[0m\n",
      "\u001b[1m\u001b[36m    | Statement                                                   |\u001b[0m\n",
      "\u001b[1m\u001b[36m    +-------------------------------------------------------------+\u001b[0m\n",
      "\u001b[1m\u001b[36m    | transform_train = transforms.Compose([                      |\u001b[0m\n",
      "\u001b[1m\u001b[36m    |     transforms.Resize((224, 224)),                          |\u001b[0m\n",
      "\u001b[1m\u001b[36m    |     transforms.RandomHorizontalFlip(),                      |\u001b[0m\n",
      "\u001b[1m\u001b[36m    |     transforms.RandomRotation(10),                          |\u001b[0m\n",
      "\u001b[1m\u001b[36m    |     transforms.RandomAffine(0, shear=10, scale=(0.8, 1.2)), |\u001b[0m\n",
      "\u001b[1m\u001b[36m    |     transforms.ColorJitter(brightness=0.2, contrast=0.2,    |\u001b[0m\n",
      "\u001b[1m\u001b[36m    |     saturation=0.2),                                        |\u001b[0m\n",
      "\u001b[1m\u001b[36m    |     transforms.ToTensor(),                                  |\u001b[0m\n",
      "\u001b[1m\u001b[36m    |     transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))  |\u001b[0m\n",
      "\u001b[1m\u001b[36m    | ])                                                          |\u001b[0m\n",
      "\u001b[1m\u001b[36m    |                                                             |\u001b[0m\n",
      "\u001b[1m\u001b[36m    | transform = transforms.Compose([                            |\u001b[0m\n",
      "\u001b[1m\u001b[36m    |     transforms.Resize((224, 224)),                          |\u001b[0m\n",
      "\u001b[1m\u001b[36m    |     transforms.ToTensor(),                                  |\u001b[0m\n",
      "\u001b[1m\u001b[36m    |     transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))  |\u001b[0m\n",
      "\u001b[1m\u001b[36m    | ])                                                          |\u001b[0m\n",
      "\u001b[1m\u001b[36m    |                                                             |\u001b[0m\n",
      "\u001b[1m\u001b[36m    | training_dataset = datasets.ImageFolder(root=data_path +    |\u001b[0m\n",
      "\u001b[1m\u001b[36m    |     \"train\",                                                |\u001b[0m\n",
      "\u001b[1m\u001b[36m    |                                                             |\u001b[0m\n",
      "\u001b[1m\u001b[36m    |     transform=transform_train)                              |\u001b[0m\n",
      "\u001b[1m\u001b[36m    |                                                             |\u001b[0m\n",
      "\u001b[1m\u001b[36m    | validation_dataset = datasets.ImageFolder(root=data_path +  |\u001b[0m\n",
      "\u001b[1m\u001b[36m    |     \"val\",                                                  |\u001b[0m\n",
      "\u001b[1m\u001b[36m    |                                                             |\u001b[0m\n",
      "\u001b[1m\u001b[36m    |     transform=transform)                                    |\u001b[0m\n",
      "\u001b[1m\u001b[36m    |                                                             |\u001b[0m\n",
      "\u001b[1m\u001b[36m    | training_loader =                                           |\u001b[0m\n",
      "\u001b[1m\u001b[36m    |     torch.utils.data.DataLoader(training_dataset,           |\u001b[0m\n",
      "\u001b[1m\u001b[36m    |                                                             |\u001b[0m\n",
      "\u001b[1m\u001b[36m    |     batch_size=20,                                          |\u001b[0m\n",
      "\u001b[1m\u001b[36m    |                                               shuffle=True) |\u001b[0m\n",
      "\u001b[1m\u001b[36m    |                                                             |\u001b[0m\n",
      "\u001b[1m\u001b[36m    | validation_loader =                                         |\u001b[0m\n",
      "\u001b[1m\u001b[36m    |     torch.utils.data.DataLoader(validation_dataset,         |\u001b[0m\n",
      "\u001b[1m\u001b[36m    |                                                             |\u001b[0m\n",
      "\u001b[1m\u001b[36m    |     batch_size=20,                                          |\u001b[0m\n",
      "\u001b[1m\u001b[36m    |                                                             |\u001b[0m\n",
      "\u001b[1m\u001b[36m    |     shuffle=False)                                          |\u001b[0m\n",
      "\u001b[1m\u001b[36m    +-------------------------------------------------------------+\u001b[0m\n",
      "\u001b[1m\u001b[36m    +-----------------+-------------------------------------------+\u001b[0m\n",
      "\u001b[1m\u001b[36m    | Variable        | Value                                     |\u001b[0m\n",
      "\u001b[1m\u001b[36m    +-----------------+-------------------------------------------+\u001b[0m\n",
      "\u001b[1m\u001b[36m    | transform_train | Compose(                                  |\u001b[0m\n",
      "\u001b[1m\u001b[36m    |                 |     Resize(size=(224, 224),               |\u001b[0m\n",
      "\u001b[1m\u001b[36m    |                 |         interpolation=bilinear,           |\u001b[0m\n",
      "\u001b[1m\u001b[36m    |                 |         max_size=None, antialias=None)    |\u001b[0m\n",
      "\u001b[1m\u001b[36m    |                 |     RandomHorizontalFlip(p=0.5)           |\u001b[0m\n",
      "\u001b[1m\u001b[36m    |                 |     RandomRotation(degrees=[-10.0, 10.0], |\u001b[0m\n",
      "\u001b[1m\u001b[36m    |                 |         interpolation=nearest,            |\u001b[0m\n",
      "\u001b[1m\u001b[36m    |                 |         expand=False, fill=0)             |\u001b[0m\n",
      "\u001b[1m\u001b[36m    |                 |     RandomAffine(degrees=[0.0, 0.0],      |\u001b[0m\n",
      "\u001b[1m\u001b[36m    |                 |         scale=(0.8, 1.2), shear=[-10.0,   |\u001b[0m\n",
      "\u001b[1m\u001b[36m    |                 |         10.0])                            |\u001b[0m\n",
      "\u001b[1m\u001b[36m    |                 |     ColorJitter(brightness=[0.8, 1.2],    |\u001b[0m\n",
      "\u001b[1m\u001b[36m    |                 |         contrast=[0.8, 1.2],              |\u001b[0m\n",
      "\u001b[1m\u001b[36m    |                 |         saturation=[0.8, 1.2], hue=None)  |\u001b[0m\n",
      "\u001b[1m\u001b[36m    |                 |     ToTensor()                            |\u001b[0m\n",
      "\u001b[1m\u001b[36m    |                 |     Normalize(mean=(0.5, 0.5, 0.5),       |\u001b[0m\n",
      "\u001b[1m\u001b[36m    |                 |         std=(0.5, 0.5, 0.5))              |\u001b[0m\n",
      "\u001b[1m\u001b[36m    |                 | )                                         |\u001b[0m\n",
      "\u001b[1m\u001b[36m    | transform       | Compose(                                  |\u001b[0m\n",
      "\u001b[1m\u001b[36m    |                 |     Resize(size=(224, 224),               |\u001b[0m\n",
      "\u001b[1m\u001b[36m    |                 |         interpolation=bilinear,           |\u001b[0m\n",
      "\u001b[1m\u001b[36m    |                 |         max_size=None, antialias=None)    |\u001b[0m\n",
      "\u001b[1m\u001b[36m    |                 |     ToTensor()                            |\u001b[0m\n",
      "\u001b[1m\u001b[36m    |                 |     Normalize(mean=(0.5, 0.5, 0.5),       |\u001b[0m\n",
      "\u001b[1m\u001b[36m    |                 |         std=(0.5, 0.5, 0.5))              |\u001b[0m\n",
      "\u001b[1m\u001b[36m    |                 | )                                         |\u001b[0m\n",
      "\u001b[1m\u001b[36m    +-----------------+-------------------------------------------+\u001b[0m\n",
      "\u001b[1m\u001b[36m    +----------------------------------------+--------------------+\u001b[0m\n",
      "\u001b[1m\u001b[36m    | Expression                             | Result             |\u001b[0m\n",
      "\u001b[1m\u001b[36m    +----------------------------------------+--------------------+\u001b[0m\n",
      "\u001b[1m\u001b[36m    | len(training_loader)                   | 13                 |\u001b[0m\n",
      "\u001b[1m\u001b[36m    | len(training_loader.dataset)           | 244                |\u001b[0m\n",
      "\u001b[1m\u001b[36m    | list(training_loader)[0][0].shape      | torch.Size([20, 3, |\u001b[0m\n",
      "\u001b[1m\u001b[36m    |                                        |             224,   |\u001b[0m\n",
      "\u001b[1m\u001b[36m    |                                        |             224])  |\u001b[0m\n",
      "\u001b[1m\u001b[36m    | list(training_loader)[0][1].shape      | torch.Size([20])   |\u001b[0m\n",
      "\u001b[1m\u001b[36m    | next(iter(training_loader))[0].shape   | torch.Size([20, 3, |\u001b[0m\n",
      "\u001b[1m\u001b[36m    |                                        |             224,   |\u001b[0m\n",
      "\u001b[1m\u001b[36m    |                                        |             224])  |\u001b[0m\n",
      "\u001b[1m\u001b[36m    | next(iter(training_loader))[1].shape   | torch.Size([20])   |\u001b[0m\n",
      "\u001b[1m\u001b[36m    | len(validation_loader)                 | 8                  |\u001b[0m\n",
      "\u001b[1m\u001b[36m    | len(validation_loader.dataset)         | 153                |\u001b[0m\n",
      "\u001b[1m\u001b[36m    | list(validation_loader)[0][0].shape    | torch.Size([20, 3, |\u001b[0m\n",
      "\u001b[1m\u001b[36m    |                                        |             224,   |\u001b[0m\n",
      "\u001b[1m\u001b[36m    |                                        |             224])  |\u001b[0m\n",
      "\u001b[1m\u001b[36m    | list(validation_loader)[0][1].shape    | torch.Size([20])   |\u001b[0m\n",
      "\u001b[1m\u001b[36m    | next(iter(validation_loader))[0].shape | torch.Size([20, 3, |\u001b[0m\n",
      "\u001b[1m\u001b[36m    |                                        |             224,   |\u001b[0m\n",
      "\u001b[1m\u001b[36m    |                                        |             224])  |\u001b[0m\n",
      "\u001b[1m\u001b[36m    | next(iter(validation_loader))[1].shape | torch.Size([20])   |\u001b[0m\n",
      "\u001b[1m\u001b[36m    +----------------------------------------+--------------------+\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "transform_train = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.RandomRotation(10),\n",
    "    transforms.RandomAffine(0, shear=10, scale=(0.8, 1.2)),\n",
    "    transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "])\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "])\n",
    "\n",
    "training_dataset = datasets.ImageFolder(root=data_path + \"train\",\n",
    "                                        transform=transform_train)\n",
    "\n",
    "validation_dataset = datasets.ImageFolder(root=data_path + \"val\",\n",
    "                                          transform=transform)\n",
    "\n",
    "training_loader = torch.utils.data.DataLoader(training_dataset,\n",
    "                                              batch_size=20,\n",
    "                                              shuffle=True)\n",
    "\n",
    "validation_loader = torch.utils.data.DataLoader(validation_dataset,\n",
    "                                                batch_size=20,\n",
    "                                                shuffle=False)\n",
    "\n",
    "tabulation = Form_Generator()\n",
    "tabulation.heading_printer(\n",
    "    \"Loading and transformation of local training and validation datasets\")\n",
    "\n",
    "statements = [\n",
    "    \"\"\"\n",
    "transform_train = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.RandomRotation(10),\n",
    "    transforms.RandomAffine(0, shear=10, scale=(0.8, 1.2)),\n",
    "    transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "])\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "])\n",
    "\n",
    "training_dataset = datasets.ImageFolder(root=data_path + \"train\",\n",
    "                                        transform=transform_train)\n",
    "\n",
    "validation_dataset = datasets.ImageFolder(root=data_path + \"val\",\n",
    "                                          transform=transform)\n",
    "\n",
    "training_loader = torch.utils.data.DataLoader(training_dataset,\n",
    "                                              batch_size=20,\n",
    "                                              shuffle=True)\n",
    "\n",
    "validation_loader = torch.utils.data.DataLoader(validation_dataset,\n",
    "                                                batch_size=20,\n",
    "                                                shuffle=False)\n",
    "\"\"\"\n",
    "]\n",
    "tabulation.statement_generator(statements)\n",
    "\n",
    "variables = [\n",
    "    \"transform_train\",\n",
    "    \"transform\",\n",
    "    #\"validation_dataset\",\n",
    "    #\"validation_dataset\",\n",
    "]\n",
    "values = [\n",
    "    str(transform_train),\n",
    "    str(transform),\n",
    "    #str(training_dataset),\n",
    "    #str(validation_dataset),\n",
    "]\n",
    "tabulation.variable_generator(variables, values)\n",
    "\n",
    "expressions = [\n",
    "    \"len(training_loader)\", \"len(training_loader.dataset)\",\n",
    "    \"list(training_loader)[0][0].shape\", \"list(training_loader)[0][1].shape\",\n",
    "    \"next(iter(training_loader))[0].shape\",\n",
    "    \"next(iter(training_loader))[1].shape\", \"len(validation_loader)\",\n",
    "    \"len(validation_loader.dataset)\", \"list(validation_loader)[0][0].shape\",\n",
    "    \"list(validation_loader)[0][1].shape\",\n",
    "    \"next(iter(validation_loader))[0].shape\",\n",
    "    \"next(iter(validation_loader))[1].shape\"\n",
    "]\n",
    "results = [\n",
    "    str(len(training_loader)),\n",
    "    str(len(training_loader.dataset)),\n",
    "    str(list(training_loader)[0][0].shape),\n",
    "    str(list(training_loader)[0][1].shape),\n",
    "    str(next(iter(training_loader))[0].shape),\n",
    "    str(next(iter(training_loader))[1].shape),\n",
    "    str(len(validation_loader)),\n",
    "    str(len(validation_loader.dataset)),\n",
    "    str(list(validation_loader)[0][0].shape),\n",
    "    str(list(validation_loader)[0][1].shape),\n",
    "    str(next(iter(validation_loader))[0].shape),\n",
    "    str(next(iter(validation_loader))[1].shape),\n",
    "]\n",
    "tabulation.expression_generator(expressions, results, 12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1bf568e6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-20T22:46:01.082381Z",
     "start_time": "2023-01-20T22:46:01.079381Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset ImageFolder\n",
       "    Number of datapoints: 153\n",
       "    Root location: ../Datasets/ants_and_bees/val\n",
       "    StandardTransform\n",
       "Transform: Compose(\n",
       "               Resize(size=(224, 224), interpolation=bilinear, max_size=None, antialias=None)\n",
       "               ToTensor()\n",
       "               Normalize(mean=(0.5, 0.5, 0.5), std=(0.5, 0.5, 0.5))\n",
       "           )"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "validation_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "709136f2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "1987d692",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  },
  "nbTranslate": {
   "displayLangs": [
    "*"
   ],
   "hotkey": "alt-t",
   "langInMainMenu": true,
   "sourceLang": "en",
   "targetLang": "fr",
   "useGoogleTranslate": true
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "345.594px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
