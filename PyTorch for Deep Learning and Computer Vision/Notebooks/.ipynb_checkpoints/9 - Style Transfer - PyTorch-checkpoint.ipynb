{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "aaaf9137",
   "metadata": {},
   "source": [
    "# Importing Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "444b4926",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-28T22:06:13.135368Z",
     "start_time": "2023-01-28T22:06:12.075472Z"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "#from torch import nn\n",
    "#from torchvision import datasets, models, transforms\n",
    "from torchvision import models, transforms\n",
    "\n",
    "#import numpy as np\n",
    "\n",
    "#import requests\n",
    "\n",
    "import matplotlib as mpl\n",
    "#import matplotlib.gridspec as gridspec\n",
    "import matplotlib.pyplot as plt\n",
    "#from mpl_toolkits.axes_grid1.inset_locator import inset_axes\n",
    "\n",
    "from PIL import Image, ImageOps\n",
    "\n",
    "#from git.repo.base import Repo\n",
    "#from itertools import chain\n",
    "#import random\n",
    "#import reprlib\n",
    "import sys"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f62ce0f",
   "metadata": {},
   "source": [
    "# Configuring Visualization Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "98e4893c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-28T22:06:13.143735Z",
     "start_time": "2023-01-28T22:06:13.136475Z"
    }
   },
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d4c30a7d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-28T22:06:13.146465Z",
     "start_time": "2023-01-28T22:06:13.144452Z"
    }
   },
   "outputs": [],
   "source": [
    "XINHUI = \"#7a7374\"\n",
    "XUEBAI = \"#fffef9\"\n",
    "YINBAI = \"#f1f0ed\"\n",
    "YINHUI = \"#918072\"\n",
    "\n",
    "figure_size = (16, 9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c96adaa6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-28T22:06:13.150566Z",
     "start_time": "2023-01-28T22:06:13.148417Z"
    }
   },
   "outputs": [],
   "source": [
    "custom_params = {\n",
    "    \"axes.axisbelow\": True,\n",
    "    \"axes.edgecolor\": YINBAI,\n",
    "    \"axes.facecolor\": XUEBAI,\n",
    "    \"axes.grid\": True,\n",
    "    \"axes.labelcolor\": XINHUI,\n",
    "    \"axes.spines.right\": False,\n",
    "    \"axes.spines.top\": False,\n",
    "    \"axes.titlecolor\": XINHUI,\n",
    "    \"figure.edgecolor\": YINBAI,\n",
    "    \"figure.facecolor\": XUEBAI,\n",
    "    \"grid.alpha\": .8,\n",
    "    \"grid.color\": YINBAI,\n",
    "    \"grid.linestyle\": \"--\",\n",
    "    \"grid.linewidth\": 1.2,\n",
    "    \"legend.edgecolor\": YINHUI,\n",
    "    \"patch.edgecolor\": XUEBAI,\n",
    "    \"patch.force_edgecolor\": True,\n",
    "    \"text.color\": XINHUI,\n",
    "    \"xtick.color\": YINHUI,\n",
    "    \"ytick.color\": YINHUI,\n",
    "}\n",
    "\n",
    "mpl.rcParams.update(custom_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fad658d",
   "metadata": {},
   "source": [
    "# Configuring Other Notebook Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8e37712a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-28T22:06:13.153011Z",
     "start_time": "2023-01-28T22:06:13.151451Z"
    }
   },
   "outputs": [],
   "source": [
    "#reprlib_rules = reprlib.Repr()\n",
    "#reprlib_rules.maxother = 250"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83421c04",
   "metadata": {},
   "source": [
    "# Pre-installing Custom Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "22b163a3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-28T22:06:13.155235Z",
     "start_time": "2023-01-28T22:06:13.153872Z"
    }
   },
   "outputs": [],
   "source": [
    "sys.path.append(\"../\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5806d1bf",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-28T22:06:13.323004Z",
     "start_time": "2023-01-28T22:06:13.156127Z"
    }
   },
   "outputs": [],
   "source": [
    "from Modules import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ca3ace39",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-28T22:06:13.325222Z",
     "start_time": "2023-01-28T22:06:13.323988Z"
    }
   },
   "outputs": [],
   "source": [
    "#def im_convert(tensor):\n",
    "#    image = tensor.cpu().clone().detach().numpy()\n",
    "#    image = image.transpose(1, 2, 0)\n",
    "#    image = image * np.array((0.5, 0.5, 0.5)) + np.array((0.5, 0.5, 0.5))\n",
    "#    image = image.clip(0, 1)\n",
    "#    return image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6b517a41",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-28T22:06:13.327745Z",
     "start_time": "2023-01-28T22:06:13.325980Z"
    }
   },
   "outputs": [],
   "source": [
    "#def tensor_to_image(tensor):\n",
    "#    transform = transforms.ToPILImage()\n",
    "#    return transform(tensor)\n",
    "#\n",
    "#\n",
    "#def grayscale_image(image):\n",
    "#    transform = transforms.Grayscale()\n",
    "#    return transform(image)\n",
    "#\n",
    "#\n",
    "#def grayscale_image_conversion(image):\n",
    "#    transform = transforms.Compose([\n",
    "#        transforms.Resize((224, 224)),\n",
    "#        transforms.ToTensor(),\n",
    "#        transforms.Normalize((0.5, ), (0.5, ))\n",
    "#    ])\n",
    "#    return transform(image)\n",
    "#\n",
    "#\n",
    "#def grayscale_im_convert(tensor):\n",
    "#    image = tensor_to_image(tensor)\n",
    "#    image = grayscale_image(image)\n",
    "#    tensor = grayscale_image_conversion(image).permute(1, 2, 0)\n",
    "#    array = tensor.clone().detach().numpy()\n",
    "#    array = array.reshape(array.shape[0], -1)\n",
    "#    array = array * 0.5 + 0.5\n",
    "#    array = array.clip(0, 1)\n",
    "#    return array"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "192ae913",
   "metadata": {},
   "source": [
    "# Loading Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e7527834",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-28T22:06:13.331323Z",
     "start_time": "2023-01-28T22:06:13.329898Z"
    }
   },
   "outputs": [],
   "source": [
    "#data_path = \"../Datasets/ants_and_bees/\"\n",
    "#\n",
    "#try:\n",
    "#    Repo.clone_from(\"https://github.com/jaddoescad/ants_and_bees\", data_path)\n",
    "#except:\n",
    "#    NameError"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e375a284",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-12T17:56:18.222325Z",
     "start_time": "2023-01-12T17:56:18.218703Z"
    }
   },
   "source": [
    "# Practicing in Stages\n",
    "\n",
    "## VGG19"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "68459da7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-28T22:06:14.030953Z",
     "start_time": "2023-01-28T22:06:13.332032Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[4m\u001b[36mLoading the features of the pre-trained VGG19 model from PyTorch\u001b[0m\n",
      "\n",
      "\u001b[1m\u001b[36m    +--------------------------------------------------+\u001b[0m\n",
      "\u001b[1m\u001b[36m    | Statement                                        |\u001b[0m\n",
      "\u001b[1m\u001b[36m    +--------------------------------------------------+\u001b[0m\n",
      "\u001b[1m\u001b[36m    | VGG19 = models.vgg19(weights=\"DEFAULT\").features |\u001b[0m\n",
      "\u001b[1m\u001b[36m    |                                                  |\u001b[0m\n",
      "\u001b[1m\u001b[36m    | for param in VGG19.parameters():                 |\u001b[0m\n",
      "\u001b[1m\u001b[36m    |     param.requires_grad_(False)                  |\u001b[0m\n",
      "\u001b[1m\u001b[36m    +--------------------------------------------------+\u001b[0m\n",
      "\u001b[1m\u001b[36m    +----------+-------------------------------------------------+\u001b[0m\n",
      "\u001b[1m\u001b[36m    | Variable | Value                                           |\u001b[0m\n",
      "\u001b[1m\u001b[36m    +----------+-------------------------------------------------+\u001b[0m\n",
      "\u001b[1m\u001b[36m    | VGG19    | Sequential(                                     |\u001b[0m\n",
      "\u001b[1m\u001b[36m    |          |   (0): Conv2d(3, 64, kernel_size=(3, 3),        |\u001b[0m\n",
      "\u001b[1m\u001b[36m    |          |         stride=(1, 1), padding=(1, 1))          |\u001b[0m\n",
      "\u001b[1m\u001b[36m    |          |   (1): ReLU(inplace=True)                       |\u001b[0m\n",
      "\u001b[1m\u001b[36m    |          |   (2): Conv2d(64, 64, kernel_size=(3, 3),       |\u001b[0m\n",
      "\u001b[1m\u001b[36m    |          |         stride=(1, 1), padding=(1, 1))          |\u001b[0m\n",
      "\u001b[1m\u001b[36m    |          |   (3): ReLU(inplace=True)                       |\u001b[0m\n",
      "\u001b[1m\u001b[36m    |          |   (4): MaxPool2d(kernel_size=2, stride=2,       |\u001b[0m\n",
      "\u001b[1m\u001b[36m    |          |         padding=0, dilation=1, ceil_mode=False) |\u001b[0m\n",
      "\u001b[1m\u001b[36m    |          |   (5): Conv2d(64, 128, kernel_size=(3, 3),      |\u001b[0m\n",
      "\u001b[1m\u001b[36m    |          |         stride=(1, 1), padding=(1, 1))          |\u001b[0m\n",
      "\u001b[1m\u001b[36m    |          |   (6): ReLU(inplace=True)                       |\u001b[0m\n",
      "\u001b[1m\u001b[36m    |          |   (7): Conv2d(128, 128, kernel_size=(3, 3),     |\u001b[0m\n",
      "\u001b[1m\u001b[36m    |          |         stride=(1, 1), padding=(1, 1))          |\u001b[0m\n",
      "\u001b[1m\u001b[36m    |          |   (8): ReLU(inplace=True)                       |\u001b[0m\n",
      "\u001b[1m\u001b[36m    |          |   (9): MaxPool2d(kernel_size=2, stride=2,       |\u001b[0m\n",
      "\u001b[1m\u001b[36m    |          |         padding=0, dilation=1, ceil_mode=False) |\u001b[0m\n",
      "\u001b[1m\u001b[36m    |          |   (10): Conv2d(128, 256, kernel_size=(3, 3),    |\u001b[0m\n",
      "\u001b[1m\u001b[36m    |          |         stride=(1, 1), padding=(1, 1))          |\u001b[0m\n",
      "\u001b[1m\u001b[36m    |          |   (11): ReLU(inplace=True)                      |\u001b[0m\n",
      "\u001b[1m\u001b[36m    |          |   (12): Conv2d(256, 256, kernel_size=(3, 3),    |\u001b[0m\n",
      "\u001b[1m\u001b[36m    |          |         stride=(1, 1), padding=(1, 1))          |\u001b[0m\n",
      "\u001b[1m\u001b[36m    |          |   (13): ReLU(inplace=True)                      |\u001b[0m\n",
      "\u001b[1m\u001b[36m    |          |   (14): Conv2d(256, 256, kernel_size=(3, 3),    |\u001b[0m\n",
      "\u001b[1m\u001b[36m    |          |         stride=(1, 1), padding=(1, 1))          |\u001b[0m\n",
      "\u001b[1m\u001b[36m    |          |   (15): ReLU(inplace=True)                      |\u001b[0m\n",
      "\u001b[1m\u001b[36m    |          |   (16): Conv2d(256, 256, kernel_size=(3, 3),    |\u001b[0m\n",
      "\u001b[1m\u001b[36m    |          |         stride=(1, 1), padding=(1, 1))          |\u001b[0m\n",
      "\u001b[1m\u001b[36m    |          |   (17): ReLU(inplace=True)                      |\u001b[0m\n",
      "\u001b[1m\u001b[36m    |          |   (18): MaxPool2d(kernel_size=2, stride=2,      |\u001b[0m\n",
      "\u001b[1m\u001b[36m    |          |         padding=0, dilation=1, ceil_mode=False) |\u001b[0m\n",
      "\u001b[1m\u001b[36m    |          |   (19): Conv2d(256, 512, kernel_size=(3, 3),    |\u001b[0m\n",
      "\u001b[1m\u001b[36m    |          |         stride=(1, 1), padding=(1, 1))          |\u001b[0m\n",
      "\u001b[1m\u001b[36m    |          |   (20): ReLU(inplace=True)                      |\u001b[0m\n",
      "\u001b[1m\u001b[36m    |          |   (21): Conv2d(512, 512, kernel_size=(3, 3),    |\u001b[0m\n",
      "\u001b[1m\u001b[36m    |          |         stride=(1, 1), padding=(1, 1))          |\u001b[0m\n",
      "\u001b[1m\u001b[36m    |          |   (22): ReLU(inplace=True)                      |\u001b[0m\n",
      "\u001b[1m\u001b[36m    |          |   (23): Conv2d(512, 512, kernel_size=(3, 3),    |\u001b[0m\n",
      "\u001b[1m\u001b[36m    |          |         stride=(1, 1), padding=(1, 1))          |\u001b[0m\n",
      "\u001b[1m\u001b[36m    |          |   (24): ReLU(inplace=True)                      |\u001b[0m\n",
      "\u001b[1m\u001b[36m    |          |   (25): Conv2d(512, 512, kernel_size=(3, 3),    |\u001b[0m\n",
      "\u001b[1m\u001b[36m    |          |         stride=(1, 1), padding=(1, 1))          |\u001b[0m\n",
      "\u001b[1m\u001b[36m    |          |   (26): ReLU(inplace=True)                      |\u001b[0m\n",
      "\u001b[1m\u001b[36m    |          |   (27): MaxPool2d(kernel_size=2, stride=2,      |\u001b[0m\n",
      "\u001b[1m\u001b[36m    |          |         padding=0, dilation=1, ceil_mode=False) |\u001b[0m\n",
      "\u001b[1m\u001b[36m    |          |   (28): Conv2d(512, 512, kernel_size=(3, 3),    |\u001b[0m\n",
      "\u001b[1m\u001b[36m    |          |         stride=(1, 1), padding=(1, 1))          |\u001b[0m\n",
      "\u001b[1m\u001b[36m    |          |   (29): ReLU(inplace=True)                      |\u001b[0m\n",
      "\u001b[1m\u001b[36m    |          |   (30): Conv2d(512, 512, kernel_size=(3, 3),    |\u001b[0m\n",
      "\u001b[1m\u001b[36m    |          |         stride=(1, 1), padding=(1, 1))          |\u001b[0m\n",
      "\u001b[1m\u001b[36m    |          |   (31): ReLU(inplace=True)                      |\u001b[0m\n",
      "\u001b[1m\u001b[36m    |          |   (32): Conv2d(512, 512, kernel_size=(3, 3),    |\u001b[0m\n",
      "\u001b[1m\u001b[36m    |          |         stride=(1, 1), padding=(1, 1))          |\u001b[0m\n",
      "\u001b[1m\u001b[36m    |          |   (33): ReLU(inplace=True)                      |\u001b[0m\n",
      "\u001b[1m\u001b[36m    |          |   (34): Conv2d(512, 512, kernel_size=(3, 3),    |\u001b[0m\n",
      "\u001b[1m\u001b[36m    |          |         stride=(1, 1), padding=(1, 1))          |\u001b[0m\n",
      "\u001b[1m\u001b[36m    |          |   (35): ReLU(inplace=True)                      |\u001b[0m\n",
      "\u001b[1m\u001b[36m    |          |   (36): MaxPool2d(kernel_size=2, stride=2,      |\u001b[0m\n",
      "\u001b[1m\u001b[36m    |          |         padding=0, dilation=1, ceil_mode=False) |\u001b[0m\n",
      "\u001b[1m\u001b[36m    |          | )                                               |\u001b[0m\n",
      "\u001b[1m\u001b[36m    +----------+-------------------------------------------------+\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "VGG19 = models.vgg19(weights=\"DEFAULT\").features\n",
    "\n",
    "for param in VGG19.parameters():\n",
    "    param.requires_grad_(False)\n",
    "\n",
    "tabulation = Form_Generator()\n",
    "tabulation.heading_printer(\n",
    "    \"Loading the features of the pre-trained VGG19 model from PyTorch\")\n",
    "\n",
    "statements = [\n",
    "    \"\"\"\n",
    "VGG19 = models.vgg19(weights=\"DEFAULT\").features\n",
    "\n",
    "for param in VGG19.parameters():\n",
    "    param.requires_grad_(False)\n",
    "\"\"\"\n",
    "]\n",
    "tabulation.statement_generator(statements)\n",
    "\n",
    "variables = [\"VGG19\"]\n",
    "values = [str(VGG19)]\n",
    "tabulation.variable_generator(variables, values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1bea8e99",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-28T22:06:14.071730Z",
     "start_time": "2023-01-28T22:06:14.032187Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[4m\u001b[30mEnabling GPU training acceleration for the pre-trained VGG19 model\u001b[0m\n",
      "\n",
      "\u001b[1m\u001b[30m    +---------------------------------------------------+\u001b[0m\n",
      "\u001b[1m\u001b[30m    | Statement                                         |\u001b[0m\n",
      "\u001b[1m\u001b[30m    +---------------------------------------------------+\u001b[0m\n",
      "\u001b[1m\u001b[30m    | device = torch.device(\"mps:0\" if                  |\u001b[0m\n",
      "\u001b[1m\u001b[30m    |     torch.backends.mps.is_available() else \"cpu\") |\u001b[0m\n",
      "\u001b[1m\u001b[30m    |                                                   |\u001b[0m\n",
      "\u001b[1m\u001b[30m    | model = VGG19.to(device)                          |\u001b[0m\n",
      "\u001b[1m\u001b[30m    +---------------------------------------------------+\u001b[0m\n",
      "\u001b[1m\u001b[30m    +----------+-------------------------------------------------+\u001b[0m\n",
      "\u001b[1m\u001b[30m    | Variable | Value                                           |\u001b[0m\n",
      "\u001b[1m\u001b[30m    +----------+-------------------------------------------------+\u001b[0m\n",
      "\u001b[1m\u001b[30m    | device   | mps:0                                           |\u001b[0m\n",
      "\u001b[1m\u001b[30m    | model    | Sequential(                                     |\u001b[0m\n",
      "\u001b[1m\u001b[30m    |          |   (0): Conv2d(3, 64, kernel_size=(3, 3),        |\u001b[0m\n",
      "\u001b[1m\u001b[30m    |          |         stride=(1, 1), padding=(1, 1))          |\u001b[0m\n",
      "\u001b[1m\u001b[30m    |          |   (1): ReLU(inplace=True)                       |\u001b[0m\n",
      "\u001b[1m\u001b[30m    |          |   (2): Conv2d(64, 64, kernel_size=(3, 3),       |\u001b[0m\n",
      "\u001b[1m\u001b[30m    |          |         stride=(1, 1), padding=(1, 1))          |\u001b[0m\n",
      "\u001b[1m\u001b[30m    |          |   (3): ReLU(inplace=True)                       |\u001b[0m\n",
      "\u001b[1m\u001b[30m    |          |   (4): MaxPool2d(kernel_size=2, stride=2,       |\u001b[0m\n",
      "\u001b[1m\u001b[30m    |          |         padding=0, dilation=1, ceil_mode=False) |\u001b[0m\n",
      "\u001b[1m\u001b[30m    |          |   (5): Conv2d(64, 128, kernel_size=(3, 3),      |\u001b[0m\n",
      "\u001b[1m\u001b[30m    |          |         stride=(1, 1), padding=(1, 1))          |\u001b[0m\n",
      "\u001b[1m\u001b[30m    |          |   (6): ReLU(inplace=True)                       |\u001b[0m\n",
      "\u001b[1m\u001b[30m    |          |   (7): Conv2d(128, 128, kernel_size=(3, 3),     |\u001b[0m\n",
      "\u001b[1m\u001b[30m    |          |         stride=(1, 1), padding=(1, 1))          |\u001b[0m\n",
      "\u001b[1m\u001b[30m    |          |   (8): ReLU(inplace=True)                       |\u001b[0m\n",
      "\u001b[1m\u001b[30m    |          |   (9): MaxPool2d(kernel_size=2, stride=2,       |\u001b[0m\n",
      "\u001b[1m\u001b[30m    |          |         padding=0, dilation=1, ceil_mode=False) |\u001b[0m\n",
      "\u001b[1m\u001b[30m    |          |   (10): Conv2d(128, 256, kernel_size=(3, 3),    |\u001b[0m\n",
      "\u001b[1m\u001b[30m    |          |         stride=(1, 1), padding=(1, 1))          |\u001b[0m\n",
      "\u001b[1m\u001b[30m    |          |   (11): ReLU(inplace=True)                      |\u001b[0m\n",
      "\u001b[1m\u001b[30m    |          |   (12): Conv2d(256, 256, kernel_size=(3, 3),    |\u001b[0m\n",
      "\u001b[1m\u001b[30m    |          |         stride=(1, 1), padding=(1, 1))          |\u001b[0m\n",
      "\u001b[1m\u001b[30m    |          |   (13): ReLU(inplace=True)                      |\u001b[0m\n",
      "\u001b[1m\u001b[30m    |          |   (14): Conv2d(256, 256, kernel_size=(3, 3),    |\u001b[0m\n",
      "\u001b[1m\u001b[30m    |          |         stride=(1, 1), padding=(1, 1))          |\u001b[0m\n",
      "\u001b[1m\u001b[30m    |          |   (15): ReLU(inplace=True)                      |\u001b[0m\n",
      "\u001b[1m\u001b[30m    |          |   (16): Conv2d(256, 256, kernel_size=(3, 3),    |\u001b[0m\n",
      "\u001b[1m\u001b[30m    |          |         stride=(1, 1), padding=(1, 1))          |\u001b[0m\n",
      "\u001b[1m\u001b[30m    |          |   (17): ReLU(inplace=True)                      |\u001b[0m\n",
      "\u001b[1m\u001b[30m    |          |   (18): MaxPool2d(kernel_size=2, stride=2,      |\u001b[0m\n",
      "\u001b[1m\u001b[30m    |          |         padding=0, dilation=1, ceil_mode=False) |\u001b[0m\n",
      "\u001b[1m\u001b[30m    |          |   (19): Conv2d(256, 512, kernel_size=(3, 3),    |\u001b[0m\n",
      "\u001b[1m\u001b[30m    |          |         stride=(1, 1), padding=(1, 1))          |\u001b[0m\n",
      "\u001b[1m\u001b[30m    |          |   (20): ReLU(inplace=True)                      |\u001b[0m\n",
      "\u001b[1m\u001b[30m    |          |   (21): Conv2d(512, 512, kernel_size=(3, 3),    |\u001b[0m\n",
      "\u001b[1m\u001b[30m    |          |         stride=(1, 1), padding=(1, 1))          |\u001b[0m\n",
      "\u001b[1m\u001b[30m    |          |   (22): ReLU(inplace=True)                      |\u001b[0m\n",
      "\u001b[1m\u001b[30m    |          |   (23): Conv2d(512, 512, kernel_size=(3, 3),    |\u001b[0m\n",
      "\u001b[1m\u001b[30m    |          |         stride=(1, 1), padding=(1, 1))          |\u001b[0m\n",
      "\u001b[1m\u001b[30m    |          |   (24): ReLU(inplace=True)                      |\u001b[0m\n",
      "\u001b[1m\u001b[30m    |          |   (25): Conv2d(512, 512, kernel_size=(3, 3),    |\u001b[0m\n",
      "\u001b[1m\u001b[30m    |          |         stride=(1, 1), padding=(1, 1))          |\u001b[0m\n",
      "\u001b[1m\u001b[30m    |          |   (26): ReLU(inplace=True)                      |\u001b[0m\n",
      "\u001b[1m\u001b[30m    |          |   (27): MaxPool2d(kernel_size=2, stride=2,      |\u001b[0m\n",
      "\u001b[1m\u001b[30m    |          |         padding=0, dilation=1, ceil_mode=False) |\u001b[0m\n",
      "\u001b[1m\u001b[30m    |          |   (28): Conv2d(512, 512, kernel_size=(3, 3),    |\u001b[0m\n",
      "\u001b[1m\u001b[30m    |          |         stride=(1, 1), padding=(1, 1))          |\u001b[0m\n",
      "\u001b[1m\u001b[30m    |          |   (29): ReLU(inplace=True)                      |\u001b[0m\n",
      "\u001b[1m\u001b[30m    |          |   (30): Conv2d(512, 512, kernel_size=(3, 3),    |\u001b[0m\n",
      "\u001b[1m\u001b[30m    |          |         stride=(1, 1), padding=(1, 1))          |\u001b[0m\n",
      "\u001b[1m\u001b[30m    |          |   (31): ReLU(inplace=True)                      |\u001b[0m\n",
      "\u001b[1m\u001b[30m    |          |   (32): Conv2d(512, 512, kernel_size=(3, 3),    |\u001b[0m\n",
      "\u001b[1m\u001b[30m    |          |         stride=(1, 1), padding=(1, 1))          |\u001b[0m\n",
      "\u001b[1m\u001b[30m    |          |   (33): ReLU(inplace=True)                      |\u001b[0m\n",
      "\u001b[1m\u001b[30m    |          |   (34): Conv2d(512, 512, kernel_size=(3, 3),    |\u001b[0m\n",
      "\u001b[1m\u001b[30m    |          |         stride=(1, 1), padding=(1, 1))          |\u001b[0m\n",
      "\u001b[1m\u001b[30m    |          |   (35): ReLU(inplace=True)                      |\u001b[0m\n",
      "\u001b[1m\u001b[30m    |          |   (36): MaxPool2d(kernel_size=2, stride=2,      |\u001b[0m\n",
      "\u001b[1m\u001b[30m    |          |         padding=0, dilation=1, ceil_mode=False) |\u001b[0m\n",
      "\u001b[1m\u001b[30m    |          | )                                               |\u001b[0m\n",
      "\u001b[1m\u001b[30m    +----------+-------------------------------------------------+\u001b[0m\n",
      "\u001b[1m\u001b[30m    +--------------+--------+\u001b[0m\n",
      "\u001b[1m\u001b[30m    | Expression   | Result |\u001b[0m\n",
      "\u001b[1m\u001b[30m    +--------------+--------+\u001b[0m\n",
      "\u001b[1m\u001b[30m    | device.index | 0      |\u001b[0m\n",
      "\u001b[1m\u001b[30m    +--------------+--------+\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"mps:0\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "\n",
    "model = VGG19.to(device)\n",
    "\n",
    "tabulation = Form_Generator()\n",
    "tabulation.heading_printer(\n",
    "    \"Enabling GPU training acceleration for the pre-trained VGG19 model\")\n",
    "\n",
    "statements = [\n",
    "    \"\"\"\n",
    "device = torch.device(\"mps:0\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "\n",
    "model = VGG19.to(device)\n",
    "\"\"\"\n",
    "]\n",
    "tabulation.statement_generator(statements)\n",
    "\n",
    "variables = [\"device\", \"model\"]\n",
    "values = [str(device), str(model)]\n",
    "tabulation.variable_generator(variables, values)\n",
    "\n",
    "expressions = [\"device.index\"]\n",
    "results = [str(device.index)]\n",
    "tabulation.expression_generator(expressions, results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c28b98c",
   "metadata": {},
   "source": [
    "## Image Transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "90fdc470",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-28T22:06:14.076550Z",
     "start_time": "2023-01-28T22:06:14.073094Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[4m\u001b[35mFunction definition to load an image from a specified file path\u001b[0m\n",
      "\n",
      "\u001b[1m\u001b[35m    +----------------------------------------------------------+\u001b[0m\n",
      "\u001b[1m\u001b[35m    | Definition                                               |\u001b[0m\n",
      "\u001b[1m\u001b[35m    +----------------------------------------------------------+\u001b[0m\n",
      "\u001b[1m\u001b[35m    | def load_image(img_path, max_size=400, shape=None):      |\u001b[0m\n",
      "\u001b[1m\u001b[35m    |     image = Image.open(img_path).convert(\"RGB\")          |\u001b[0m\n",
      "\u001b[1m\u001b[35m    |                                                          |\u001b[0m\n",
      "\u001b[1m\u001b[35m    |     if max(image.size) > max_size:                       |\u001b[0m\n",
      "\u001b[1m\u001b[35m    |         size = max_size                                  |\u001b[0m\n",
      "\u001b[1m\u001b[35m    |     else:                                                |\u001b[0m\n",
      "\u001b[1m\u001b[35m    |         size = max(image.size)                           |\u001b[0m\n",
      "\u001b[1m\u001b[35m    |                                                          |\u001b[0m\n",
      "\u001b[1m\u001b[35m    |     in_transform = transforms.Compose([                  |\u001b[0m\n",
      "\u001b[1m\u001b[35m    |         transforms.Resize(size),                         |\u001b[0m\n",
      "\u001b[1m\u001b[35m    |         transforms.ToTensor(),                           |\u001b[0m\n",
      "\u001b[1m\u001b[35m    |         transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, |\u001b[0m\n",
      "\u001b[1m\u001b[35m    |     0.5))                                                |\u001b[0m\n",
      "\u001b[1m\u001b[35m    |     ])                                                   |\u001b[0m\n",
      "\u001b[1m\u001b[35m    |                                                          |\u001b[0m\n",
      "\u001b[1m\u001b[35m    |     image = in_transform(image).unsqueeze(0)             |\u001b[0m\n",
      "\u001b[1m\u001b[35m    |                                                          |\u001b[0m\n",
      "\u001b[1m\u001b[35m    |     return image                                         |\u001b[0m\n",
      "\u001b[1m\u001b[35m    +----------------------------------------------------------+\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "def load_image(img_path, max_size=400, shape=None):\n",
    "    image = Image.open(img_path).convert(\"RGB\")\n",
    "\n",
    "    if max(image.size) > max_size:\n",
    "        size = max_size\n",
    "    else:\n",
    "        size = max(image.size)\n",
    "\n",
    "    in_transform = transforms.Compose([\n",
    "        transforms.Resize(size),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "    ])\n",
    "\n",
    "    image = in_transform(image).unsqueeze(0)\n",
    "\n",
    "    return image\n",
    "\n",
    "\n",
    "tabulation = Form_Generator()\n",
    "tabulation.heading_printer(\n",
    "    \"Function definition to load an image from a specified file path\")\n",
    "\n",
    "definitions = [\n",
    "    \"\"\"\n",
    "def load_image(img_path, max_size=400, shape=None):\n",
    "    image = Image.open(img_path).convert(\"RGB\")\n",
    "\n",
    "    if max(image.size) > max_size:\n",
    "        size = max_size\n",
    "    else:\n",
    "        size = max(image.size)\n",
    "\n",
    "    in_transform = transforms.Compose([\n",
    "        transforms.Resize(size),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "    ])\n",
    "\n",
    "    image = in_transform(image).unsqueeze(0)\n",
    "\n",
    "    return image\n",
    "\"\"\"\n",
    "]\n",
    "tabulation.definition_generator(definitions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4d70058",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf41fab3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbd9a61c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54bc2455",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-27T01:48:43.917025Z",
     "start_time": "2023-01-27T01:48:43.914658Z"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ff47a5ea",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-24T12:50:11.440564Z",
     "start_time": "2023-01-24T12:50:07.659454Z"
    },
    "scrolled": false
   },
   "source": [
    "transform_train = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.RandomAffine(0, shear=10, scale=(0.8, 1.2)),\n",
    "    transforms.ColorJitter(brightness=1, contrast=1, saturation=1),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "])\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "])\n",
    "\n",
    "training_set = datasets.ImageFolder(root=data_path + \"train\",\n",
    "                                    transform=transform_train)\n",
    "\n",
    "validation_set = datasets.ImageFolder(root=data_path + \"val\",\n",
    "                                      transform=transform)\n",
    "\n",
    "training_loader = torch.utils.data.DataLoader(training_set,\n",
    "                                              batch_size=20,\n",
    "                                              shuffle=True)\n",
    "\n",
    "validation_loader = torch.utils.data.DataLoader(validation_set,\n",
    "                                                batch_size=20,\n",
    "                                                shuffle=False)\n",
    "\n",
    "tabulation = Form_Generator()\n",
    "tabulation.heading_printer(\n",
    "    \"Loading and transformation of local training and validation datasets\")\n",
    "\n",
    "statements = [\n",
    "    \"\"\"\n",
    "transform_train = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.RandomAffine(0, shear=10, scale=(0.8, 1.2)),\n",
    "    transforms.ColorJitter(brightness=1, contrast=1, saturation=1),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "])\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "])\n",
    "\n",
    "training_set = datasets.ImageFolder(root=data_path + \"train\",\n",
    "                                    transform=transform_train)\n",
    "\n",
    "validation_set = datasets.ImageFolder(root=data_path + \"val\",\n",
    "                                      transform=transform)\n",
    "\n",
    "training_loader = torch.utils.data.DataLoader(training_set,\n",
    "                                              batch_size=20,\n",
    "                                              shuffle=True)\n",
    "\n",
    "validation_loader = torch.utils.data.DataLoader(validation_set,\n",
    "                                                batch_size=20,\n",
    "                                                shuffle=False)\n",
    "\"\"\"\n",
    "]\n",
    "tabulation.long_statement_generator(statements)\n",
    "\n",
    "variables = [\n",
    "    \"transform_train\",\n",
    "    \"transform\",\n",
    "    \"training_set\",\n",
    "    \"validation_set\",\n",
    "]\n",
    "values = [\n",
    "    str(transform_train),\n",
    "    str(transform),\n",
    "    str(reprlib_rules.repr(training_set)),\n",
    "    str(reprlib_rules.repr(validation_set)),\n",
    "]\n",
    "tabulation.variable_generator(variables, values)\n",
    "\n",
    "expressions = [\n",
    "    \"len(training_set)\", \"len(validation_set))\", \"len(training_loader)\",\n",
    "    \"len(training_loader.dataset)\", \"list(training_loader)[0][0].shape\",\n",
    "    \"list(training_loader)[0][1].shape\",\n",
    "    \"next(iter(training_loader))[0].shape\",\n",
    "    \"next(iter(training_loader))[1].shape\", \"len(validation_loader)\",\n",
    "    \"len(validation_loader.dataset)\", \"list(validation_loader)[0][0].shape\",\n",
    "    \"list(validation_loader)[0][1].shape\",\n",
    "    \"next(iter(validation_loader))[0].shape\",\n",
    "    \"next(iter(validation_loader))[1].shape\"\n",
    "]\n",
    "results = [\n",
    "    str(len(training_set)),\n",
    "    str(len(validation_set)),\n",
    "    str(len(training_loader)),\n",
    "    str(len(training_loader.dataset)),\n",
    "    str(list(training_loader)[0][0].shape),\n",
    "    str(list(training_loader)[0][1].shape),\n",
    "    str(next(iter(training_loader))[0].shape),\n",
    "    str(next(iter(training_loader))[1].shape),\n",
    "    str(len(validation_loader)),\n",
    "    str(len(validation_loader.dataset)),\n",
    "    str(list(validation_loader)[0][0].shape),\n",
    "    str(list(validation_loader)[0][1].shape),\n",
    "    str(next(iter(validation_loader))[0].shape),\n",
    "    str(next(iter(validation_loader))[1].shape),\n",
    "]\n",
    "tabulation.expression_generator(expressions, results, 12)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46b2096b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-24T12:50:12.783398Z",
     "start_time": "2023-01-24T12:50:11.441475Z"
    }
   },
   "source": [
    "colors = calm_color_generator(20)\n",
    "\n",
    "classes = (\"ant\", \"bee\")\n",
    "\n",
    "fig = plt.figure(figsize=(figure_size[0], figure_size[1] / 9 * 11),\n",
    "                 constrained_layout=True)\n",
    "\n",
    "gs = gridspec.GridSpec(nrows=4, ncols=5, figure=fig, wspace=.08, hspace=None)\n",
    "\n",
    "dataiter = iter(training_loader)\n",
    "images, labels = next(dataiter)\n",
    "\n",
    "for idx in np.arange(20):\n",
    "    bgcolor = colors.pop(0)\n",
    "    ax = fig.add_subplot(gs[idx // 5, idx % 5],\n",
    "                         xticks=[],\n",
    "                         yticks=[],\n",
    "                         frameon=False)\n",
    "    title = classes[labels[idx].item()]\n",
    "    if title in [\"ant\"]:\n",
    "        title = title.center(51 - len(title))\n",
    "    else:\n",
    "        title = title.center(50 - len(title))\n",
    "    ax.set_title(title,\n",
    "                 loc=\"center\",\n",
    "                 pad=0,\n",
    "                 backgroundcolor=bgcolor,\n",
    "                 color=plt.rcParams[\"axes.facecolor\"],\n",
    "                 fontweight=\"heavy\")\n",
    "\n",
    "    axins1 = inset_axes(ax,\n",
    "                        width=\"50%\",\n",
    "                        height=\"75%\",\n",
    "                        loc=\"upper left\",\n",
    "                        borderpad=0)\n",
    "    axins1.set(xticks=[], yticks=[], frame_on=False)\n",
    "    axins1.imshow(im_convert(images[idx]))\n",
    "    axins1.set_xlabel(\"[original]\",\n",
    "                      loc=\"left\",\n",
    "                      labelpad=5,\n",
    "                      color=bgcolor,\n",
    "                      fontweight=\"bold\")\n",
    "\n",
    "    axins2 = inset_axes(ax,\n",
    "                        width=\"50%\",\n",
    "                        height=\"75%\",\n",
    "                        loc=\"upper right\",\n",
    "                        borderpad=0)\n",
    "    axins2.set(xticks=[], yticks=[], frame_on=False)\n",
    "    axins2.imshow(grayscale_im_convert(images[idx]), cmap=\"binary_r\")\n",
    "    axins2.set_xlabel(\"[grayscale]\",\n",
    "                      loc=\"right\",\n",
    "                      labelpad=5,\n",
    "                      color=bgcolor,\n",
    "                      fontweight=\"bold\")\n",
    "\n",
    "fig.suptitle(\n",
    "    \"Plotting of the 20 shuffled images of ants and bees in the first batch\",\n",
    "    fontsize=\"x-large\",\n",
    "    x=0.5,\n",
    "    y=0)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa64ead7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-24T12:50:13.020004Z",
     "start_time": "2023-01-24T12:50:12.784494Z"
    },
    "scrolled": false
   },
   "source": [
    "AlexNet = models.alexnet(weights=\"DEFAULT\")\n",
    "\n",
    "tabulation = Form_Generator()\n",
    "tabulation.heading_printer(\n",
    "    \"Loading the pre-trained AlexNet model from PyTorch\")\n",
    "\n",
    "statements = [\"\"\"\n",
    "AlexNet = models.alexnet(weights=\"DEFAULT\")\n",
    "\"\"\"]\n",
    "tabulation.statement_generator(statements)\n",
    "\n",
    "variables = [\"AlexNet\"]\n",
    "values = [str(AlexNet)]\n",
    "tabulation.variable_generator(variables, values)\n",
    "\n",
    "expressions = [\n",
    "    \"len(AlexNet.features)\", \"len(AlexNet.classifier)\",\n",
    "    \"AlexNet.classifier[6].in_features\", \"AlexNet.classifier[6].out_features\"\n",
    "]\n",
    "results = [\n",
    "    str(len(AlexNet.features)),\n",
    "    str(len(AlexNet.classifier)),\n",
    "    str(AlexNet.classifier[6].in_features),\n",
    "    str(AlexNet.classifier[6].out_features)\n",
    "]\n",
    "tabulation.expression_generator(expressions, results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2343bc13",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-24T12:50:13.025705Z",
     "start_time": "2023-01-24T12:50:13.021093Z"
    },
    "scrolled": false
   },
   "source": [
    "for param in AlexNet.features.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "n_inputs = AlexNet.classifier[6].in_features\n",
    "last_layer = nn.Linear(n_inputs, len(classes))\n",
    "AlexNet.classifier[6] = last_layer\n",
    "\n",
    "tabulation = Form_Generator()\n",
    "tabulation.heading_printer(\n",
    "    \"Modification of pre-trained AlexNet model to fit local datasets\")\n",
    "\n",
    "statements = [\n",
    "    \"\"\"\n",
    "for param in AlexNet.features.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "n_inputs = AlexNet.classifier[6].in_features\n",
    "last_layer = nn.Linear(n_inputs, len(classes))\n",
    "AlexNet.classifier[6] = last_layer\n",
    "\"\"\"\n",
    "]\n",
    "tabulation.statement_generator(statements)\n",
    "\n",
    "variables = [\"n_inputs\", \"last_layer\", \"AlexNet\"]\n",
    "values = [str(n_inputs), str(last_layer), str(AlexNet)]\n",
    "tabulation.variable_generator(variables, values)\n",
    "\n",
    "expressions = [\n",
    "    \"len(AlexNet.features)\", \"len(AlexNet.classifier)\",\n",
    "    \"AlexNet.classifier[6].in_features\", \"AlexNet.classifier[6].out_features\"\n",
    "]\n",
    "results = [\n",
    "    str(len(AlexNet.features)),\n",
    "    str(len(AlexNet.classifier)),\n",
    "    str(AlexNet.classifier[6].in_features),\n",
    "    str(AlexNet.classifier[6].out_features)\n",
    "]\n",
    "tabulation.expression_generator(expressions, results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "582effd6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-24T12:50:13.097425Z",
     "start_time": "2023-01-24T12:50:13.026571Z"
    }
   },
   "source": [
    "model = AlexNet.to(device)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.0001)\n",
    "\n",
    "tabulation = Form_Generator()\n",
    "tabulation.heading_printer(\n",
    "    \"Initialization of neural module, criterion and optimizer\")\n",
    "\n",
    "statements = [\n",
    "    \"\"\"\n",
    "model = AlexNet.to(device)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.0001)\n",
    "\"\"\"\n",
    "]\n",
    "tabulation.statement_generator(statements)\n",
    "\n",
    "variables = [\"model\", \"criterion\", \"optimizer\"]\n",
    "values = [str(model), str(criterion), str(optimizer)]\n",
    "tabulation.variable_generator(variables, values)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ee8cdfc",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-24T12:50:35.598384Z",
     "start_time": "2023-01-24T12:50:13.099923Z"
    },
    "scrolled": false
   },
   "source": [
    "epochs = 10\n",
    "running_loss_history = []\n",
    "running_accu_history = []\n",
    "val_running_loss_history = []\n",
    "val_running_accu_history = []\n",
    "\n",
    "dataframe = DataFrame_Generator(\"epoch\", \"loss\", \"accuracy\", \"validation loss\",\n",
    "                                \"validation accuracy\")\n",
    "\n",
    "for e in range(epochs):\n",
    "    running_loss = 0.0\n",
    "    running_accu = 0\n",
    "    val_running_loss = 0.0\n",
    "    val_running_accu = 0\n",
    "\n",
    "    for inputs, labels in training_loader:\n",
    "        inputs = inputs.to(device)\n",
    "        labels = labels.to(device)\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        _, preds = torch.max(outputs, 1)\n",
    "        running_loss += loss.item()\n",
    "        running_accu += torch.sum(preds == labels.data)\n",
    "\n",
    "    else:\n",
    "        with torch.no_grad():\n",
    "            for val_inputs, val_labels in validation_loader:\n",
    "                val_inputs = val_inputs.to(device)\n",
    "                val_labels = val_labels.to(device)\n",
    "                val_outputs = model(val_inputs)\n",
    "                val_loss = criterion(val_outputs, val_labels)\n",
    "\n",
    "                _, val_preds = torch.max(val_outputs, 1)\n",
    "                val_running_loss += val_loss.item()\n",
    "                val_running_accu += torch.sum(val_preds == val_labels.data)\n",
    "\n",
    "        epoch_loss = running_loss / len(training_loader)\n",
    "        epoch_accu = running_accu.float() / len(training_loader.dataset) * 100\n",
    "        running_loss_history.append(epoch_loss)\n",
    "        running_accu_history.append(epoch_accu.item())\n",
    "\n",
    "        val_epoch_loss = val_running_loss / len(validation_loader)\n",
    "        val_epoch_accu = val_running_accu.float() / len(\n",
    "            validation_loader.dataset) * 100\n",
    "        val_running_loss_history.append(val_epoch_loss)\n",
    "        val_running_accu_history.append(val_epoch_accu.item())\n",
    "\n",
    "        dataframe.updater(e + 1, epoch_loss, epoch_accu.item(), val_epoch_loss,\n",
    "                          val_epoch_accu.item())\n",
    "\n",
    "tabulation = Form_Generator()\n",
    "tabulation.heading_printer(\n",
    "    \"Training and testing of pre-trained AlexNet model using local datasets\")\n",
    "\n",
    "statements = [\n",
    "    \"\"\"\n",
    "epochs = 10\n",
    "running_loss_history = []\n",
    "running_accu_history = []\n",
    "val_running_loss_history = []\n",
    "val_running_accu_history = []\n",
    "\n",
    "for e in range(epochs):\n",
    "    running_loss = 0.0\n",
    "    running_accu = 0\n",
    "    val_running_loss = 0.0\n",
    "    val_running_accu = 0\n",
    "\n",
    "    for inputs, labels in training_loader:\n",
    "        inputs = inputs.to(device)\n",
    "        labels = labels.to(device)\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        _, preds = torch.max(outputs, 1)\n",
    "        running_loss += loss.item()\n",
    "        running_accu += torch.sum(preds == labels.data)\n",
    "\n",
    "    else:\n",
    "        with torch.no_grad():\n",
    "            for val_inputs, val_labels in validation_loader:\n",
    "                val_inputs = val_inputs.to(device)\n",
    "                val_labels = val_labels.to(device)\n",
    "                val_outputs = model(val_inputs)\n",
    "                val_loss = criterion(val_outputs, val_labels)\n",
    "\n",
    "                _, val_preds = torch.max(val_outputs, 1)\n",
    "                val_running_loss += val_loss.item()\n",
    "                val_running_accu += torch.sum(val_preds == val_labels.data)\n",
    "\n",
    "        epoch_loss = running_loss / len(training_loader)\n",
    "        epoch_accu = running_accu.float() / len(training_loader.dataset) * 100\n",
    "        running_loss_history.append(epoch_loss)\n",
    "        running_accu_history.append(epoch_accu.item())\n",
    "\n",
    "        val_epoch_loss = val_running_loss / len(validation_loader)\n",
    "        val_epoch_accu = val_running_accu.float() / len(\n",
    "            validation_loader.dataset) * 100\n",
    "        val_running_loss_history.append(val_epoch_loss)\n",
    "        val_running_accu_history.append(val_epoch_accu.item())\n",
    "\"\"\"\n",
    "]\n",
    "tabulation.statement_generator(statements)\n",
    "\n",
    "variables = [\n",
    "    \"epochs\", \"running_loss_history\", \"running_accu_history\",\n",
    "    \"val_running_loss_history\", \"val_running_accu_history\"\n",
    "]\n",
    "values = [\n",
    "    str(epochs),\n",
    "    str(\n",
    "        reprlib_rules.repr(\n",
    "            [np.round(loss, 4) for loss in running_loss_history])),\n",
    "    str(\n",
    "        reprlib_rules.repr(\n",
    "            [np.round(accu, 4) for accu in running_accu_history])),\n",
    "    str(\n",
    "        reprlib_rules.repr(\n",
    "            [np.round(loss, 4) for loss in val_running_loss_history])),\n",
    "    str(\n",
    "        reprlib_rules.repr(\n",
    "            [np.round(accu, 4) for accu in val_running_accu_history]))\n",
    "]\n",
    "tabulation.variable_generator(variables, values, expandtabs=1)\n",
    "\n",
    "df_table = dataframe.tabulation()\n",
    "tabulation.dataframe_generator(df_table)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8de544ca",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-24T12:50:35.989681Z",
     "start_time": "2023-01-24T12:50:35.599409Z"
    }
   },
   "source": [
    "url_ant = (\"https://live.staticflickr.com/140/\"\n",
    "           \"379379222_24e134c461_b.jpg\")\n",
    "url_bee = (\"https://live.staticflickr.com/5028/\"\n",
    "           \"5630181321_7de454e707_b.jpg\")\n",
    "\n",
    "response_ant = requests.get(url_ant, stream=True)\n",
    "img_ant = Image.open(response_ant.raw)\n",
    "\n",
    "response_bee = requests.get(url_bee, stream=True)\n",
    "img_bee = Image.open(response_bee.raw)\n",
    "\n",
    "tabulation = Form_Generator()\n",
    "tabulation.heading_printer(\"Web image grabbing\")\n",
    "\n",
    "statements = [\n",
    "    \"\"\"\n",
    "url_ant = (\"https://live.staticflickr.com/140/\"\n",
    "           \"379379222_24e134c461_b.jpg\")\n",
    "url_bee = (\"https://live.staticflickr.com/5028/\"\n",
    "           \"5630181321_7de454e707_b.jpg\")\n",
    "\n",
    "response_ant = requests.get(url_ant, stream=True)\n",
    "img_ant = Image.open(response_ant.raw)\n",
    "\n",
    "response_bee = requests.get(url_bee, stream=True)\n",
    "img_bee = Image.open(response_bee.raw)\n",
    "\"\"\"\n",
    "]\n",
    "tabulation.statement_generator(statements)\n",
    "\n",
    "variables = [\"response_ant\", \"response_bee\"]\n",
    "values = [str(response_ant), str(response_bee)]\n",
    "tabulation.variable_generator(variables, values)\n",
    "\n",
    "expressions = [\n",
    "    \"np.array(img_ant)\", \"np.array(img_ant).shape\", \"np.array(img_bee)\",\n",
    "    \"np.array(img_bee).shape\"\n",
    "]\n",
    "results = [\n",
    "    str(reprlib_rules.repr(np.array(img_ant))),\n",
    "    str(np.array(img_ant).shape),\n",
    "    str(reprlib_rules.repr(np.array(img_bee))),\n",
    "    str(np.array(img_bee).shape)\n",
    "]\n",
    "tabulation.expression_generator(expressions, results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65c22ea0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-24T12:50:37.591124Z",
     "start_time": "2023-01-24T12:50:35.991463Z"
    }
   },
   "source": [
    "plt.rcParams[\"figure.figsize\"] = (figure_size[0], figure_size[1] * 2)\n",
    "\n",
    "fig, axs = plt.subplots(4, 3)\n",
    "\n",
    "axs[0, 0].imshow(img_ant)\n",
    "axs[0, 0].set_title(\"Original image (ant)\", loc=\"center\", pad=15)\n",
    "\n",
    "inverted_img_ant = ImageOps.invert(img_ant)\n",
    "axs[0, 1].imshow(inverted_img_ant)\n",
    "axs[0, 1].set_title(\"Inverted image (ant)\", loc=\"center\", pad=15)\n",
    "\n",
    "transformed_img_ant = transform(img_ant)\n",
    "axs[0, 2].imshow(im_convert(transformed_img_ant))\n",
    "axs[0, 2].set_title(\"Transformed image (ant)\", loc=\"center\", pad=15)\n",
    "\n",
    "grayscale_img_ant = grayscale_image(img_ant)\n",
    "axs[1, 0].imshow(grayscale_img_ant, cmap=\"binary_r\")\n",
    "axs[1, 0].set_title(\"Grayscale image (ant)\", loc=\"center\", pad=15)\n",
    "\n",
    "axs[1, 1].imshow(grayscale_img_ant, cmap=\"binary\")\n",
    "axs[1, 1].set_title(\"Inverted grayscale image (ant)\", loc=\"center\", pad=15)\n",
    "\n",
    "transformed_grayscale_img_ant = grayscale_image_conversion(\n",
    "    grayscale_img_ant).permute(1, 2, 0)\n",
    "axs[1, 2].imshow(transformed_grayscale_img_ant, cmap=\"binary_r\")\n",
    "axs[1, 2].set_title(\"Transformed grayscale image (ant)\", loc=\"center\", pad=15)\n",
    "\n",
    "axs[2, 0].imshow(img_bee)\n",
    "axs[2, 0].set_title(\"Original image (bee)\", loc=\"center\", pad=15)\n",
    "\n",
    "inverted_img_bee = ImageOps.invert(img_bee)\n",
    "axs[2, 1].imshow(inverted_img_bee)\n",
    "axs[2, 1].set_title(\"Inverted image (bee)\", loc=\"center\", pad=15)\n",
    "\n",
    "transformed_img_bee = transform(img_bee)\n",
    "axs[2, 2].imshow(im_convert(transformed_img_bee))\n",
    "axs[2, 2].set_title(\"Transformed image (bee)\", loc=\"center\", pad=15)\n",
    "\n",
    "grayscale_img_bee = grayscale_image(img_bee)\n",
    "axs[3, 0].imshow(grayscale_img_bee, cmap=\"binary_r\")\n",
    "axs[3, 0].set_title(\"Grayscale image (bee)\", loc=\"center\", pad=15)\n",
    "\n",
    "axs[3, 1].imshow(grayscale_img_bee, cmap=\"binary\")\n",
    "axs[3, 1].set_title(\"Inverted grayscale image (bee)\", loc=\"center\", pad=15)\n",
    "\n",
    "transformed_grayscale_img_bee = grayscale_image_conversion(\n",
    "    grayscale_img_bee).permute(1, 2, 0)\n",
    "axs[3, 2].imshow(transformed_grayscale_img_bee, cmap=\"binary_r\")\n",
    "axs[3, 2].set_title(\"Transformed grayscale image (bee)\", loc=\"center\", pad=15)\n",
    "\n",
    "fig.suptitle(\"Visual Comparison of Different Forms of Grabbed Web Images\",\n",
    "             fontsize=\"x-large\",\n",
    "             x=0.5,\n",
    "             y=0)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dfda6da",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-24T12:50:37.730237Z",
     "start_time": "2023-01-24T12:50:37.592133Z"
    }
   },
   "source": [
    "img_ant = transform(img_ant).to(device).unsqueeze(0)\n",
    "img_bee = transform(img_bee).to(device).unsqueeze(0)\n",
    "\n",
    "output_ant = model(img_ant)\n",
    "_ant, pred_ant = torch.max(output_ant, 1)\n",
    "\n",
    "output_bee = model(img_bee)\n",
    "_bee, pred_bee = torch.max(output_bee, 1)\n",
    "\n",
    "tabulation = Form_Generator()\n",
    "tabulation.heading_printer(\n",
    "    \"Test of the classification model with web-scraped image\")\n",
    "\n",
    "statements = [\n",
    "    \"\"\"\n",
    "img_ant = transform(img_ant).to(device).unsqueeze(0)\n",
    "img_bee = transform(img_bee).to(device).unsqueeze(0)\n",
    "\n",
    "output_ant = model(img_ant)\n",
    "_ant, pred_ant = torch.max(output_ant, 1)\n",
    "\n",
    "output_bee = model(img_bee)\n",
    "_bee, pred_bee = torch.max(output_bee, 1)\n",
    "\"\"\"\n",
    "]\n",
    "tabulation.statement_generator(statements)\n",
    "\n",
    "variables = [\n",
    "    \"img_ant\", \"output_ant\", \"_ant\", \"pred_ant\", \"img_bee\", \"output_bee\",\n",
    "    \"_bee\", \"pred_bee\"\n",
    "]\n",
    "values = [\n",
    "    str(reprlib_rules.repr(img_ant.cpu())),\n",
    "    str(output_ant.cpu()),\n",
    "    str(_ant.cpu()),\n",
    "    str(pred_ant.cpu()),\n",
    "    str(reprlib_rules.repr(img_bee.cpu())),\n",
    "    str(output_bee.cpu()),\n",
    "    str(_bee.cpu()),\n",
    "    str(pred_bee.cpu())\n",
    "]\n",
    "tabulation.variable_generator(variables, values)\n",
    "\n",
    "expressions = [\n",
    "    \"img_ant.shape\", \"torch.max(output_ant, 0)\", \"torch.max(output_ant, 1)\",\n",
    "    \"pred_ant.item()\", \"classes[pred_ant.item()]\", \"img_bee.shape\",\n",
    "    \"torch.max(output_bee, 0)\", \"torch.max(output_bee, 1)\", \"pred_bee.item()\",\n",
    "    \"classes[pred_bee.item()]\"\n",
    "]\n",
    "results = [\n",
    "    str(img_ant.shape),\n",
    "    str(torch.max(output_ant.cpu(), 0)),\n",
    "    str(torch.max(output_ant.cpu(), 1)),\n",
    "    str(pred_ant.item()), classes[pred_ant.item()],\n",
    "    str(img_bee.shape),\n",
    "    str(torch.max(output_bee.cpu(), 0)),\n",
    "    str(torch.max(output_bee.cpu(), 1)),\n",
    "    str(pred_bee.item()), classes[pred_bee.item()]\n",
    "]\n",
    "tabulation.expression_generator(expressions, results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62d900f5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-24T12:52:07.018663Z",
     "start_time": "2023-01-24T12:50:37.731191Z"
    }
   },
   "source": [
    "def loader_training_iterations_CNN(lr, criterion, optimizer_type, **kwargs):\n",
    "    criterion = criterion\n",
    "    optimizer = optimizer_type(model.parameters(), lr, **kwargs)\n",
    "    running_loss = 0.0\n",
    "    running_accu = 0\n",
    "    for inputs, labels in training_loader:\n",
    "        inputs = inputs.to(device)\n",
    "        labels = labels.to(device)\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        _, preds = torch.max(outputs, 1)\n",
    "        running_loss += loss.item()\n",
    "        running_accu += torch.sum(preds == labels.data)\n",
    "    else:\n",
    "        epoch_loss = running_loss / len(training_loader)\n",
    "        epoch_accu = running_accu.float() / len(training_loader.dataset) * 100\n",
    "    return epoch_loss, epoch_accu\n",
    "\n",
    "\n",
    "def loader_validation_iterations_CNN(lr, criterion):\n",
    "    criterion = criterion\n",
    "    val_running_loss = 0.0\n",
    "    val_running_accu = 0\n",
    "    with torch.no_grad():\n",
    "        for val_inputs, val_labels in validation_loader:\n",
    "            val_inputs = val_inputs.to(device)\n",
    "            val_labels = val_labels.to(device)\n",
    "            val_outputs = model(val_inputs)\n",
    "            val_loss = criterion(val_outputs, val_labels)\n",
    "            _, val_preds = torch.max(val_outputs, 1)\n",
    "            val_running_loss += val_loss.item()\n",
    "            val_running_accu += torch.sum(val_preds == val_labels.data)\n",
    "        else:\n",
    "            val_epoch_loss = val_running_loss / len(validation_loader)\n",
    "            val_epoch_accu = val_running_accu.float() / len(\n",
    "                validation_loader.dataset) * 100\n",
    "    return val_epoch_loss, val_epoch_accu\n",
    "\n",
    "\n",
    "def validation_iterations_CNN(epochs,\n",
    "                              lr,\n",
    "                              criterion=nn.CrossEntropyLoss(),\n",
    "                              optimizer_type=torch.optim.Adam,\n",
    "                              **kwargs):\n",
    "    epochs = epochs\n",
    "    loss = []\n",
    "    accu = []\n",
    "    val_loss = []\n",
    "    val_accu = []\n",
    "    for e in range(epochs):\n",
    "        epoch_loss, epoch_accu = loader_training_iterations_CNN(\n",
    "            lr, criterion, optimizer_type, **kwargs)\n",
    "        val_epoch_loss, val_epoch_accu = loader_validation_iterations_CNN(\n",
    "            lr, criterion)\n",
    "        loss.append(epoch_loss)\n",
    "        accu.append(epoch_accu.item())\n",
    "        val_loss.append(val_epoch_loss)\n",
    "        val_accu.append(val_epoch_accu.item())\n",
    "    return loss, accu, val_loss, val_accu\n",
    "\n",
    "\n",
    "colors = calm_color_generator(16)\n",
    "plt.rcParams[\"figure.figsize\"] = (figure_size[0], figure_size[1])\n",
    "\n",
    "fig, axs = plt.subplots(2, 2, sharex=True)\n",
    "\n",
    "lr_list = [0.01, 0.001, 0.0005, 0.0001]\n",
    "\n",
    "for i in range(2):\n",
    "    for j in range(2):\n",
    "        model = AlexNet.to(device)\n",
    "        loss, accu, val_loss, val_accu = validation_iterations_CNN(\n",
    "            epochs=epochs, lr=lr_list[i * 2 + j])\n",
    "        line1 = axs[i, j].plot(range(1, epochs + 1),\n",
    "                               loss,\n",
    "                               label=\"training loss\",\n",
    "                               linestyle=\":\",\n",
    "                               c=colors.pop(0))\n",
    "        line2 = axs[i, j].plot(range(1, epochs + 1),\n",
    "                               val_loss,\n",
    "                               label=\"validation loss\",\n",
    "                               linestyle=\"-.\",\n",
    "                               c=colors.pop(0))\n",
    "        axs[i, j].set(xlabel=\"epoch\", ylabel=\"loss\")\n",
    "        axs[i, j] = axs[i, j].twinx()\n",
    "        line3 = axs[i, j].plot(range(1, epochs + 1),\n",
    "                               accu,\n",
    "                               label=\"training accuracy\",\n",
    "                               linestyle=\"--\",\n",
    "                               c=colors.pop(0))\n",
    "        line4 = axs[i, j].plot(range(1, epochs + 1),\n",
    "                               val_accu,\n",
    "                               label=\"validation accuracy\",\n",
    "                               c=colors.pop(0))\n",
    "        axs[i, j].set(ylabel=\"accuracy (%)\")\n",
    "        lines = line1 + line2 + line3 + line4\n",
    "        labels = [line.get_label() for line in lines]\n",
    "        axs[i, j].legend(lines, labels, loc=\"lower left\", borderpad=1, ncol=2)\n",
    "\n",
    "for ax, lr in zip(axs.flat, lr_list):\n",
    "    ax.set_title(\"Learning rate = {}\".format(lr), loc=\"center\", pad=15)\n",
    "\n",
    "fig.suptitle(\n",
    "    \"Visual Comparison of Model Testing with Different Learning Rates\",\n",
    "    fontsize=\"x-large\",\n",
    "    x=0.5,\n",
    "    y=0)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6de57fa",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-24T12:53:38.302757Z",
     "start_time": "2023-01-24T12:52:07.019559Z"
    }
   },
   "source": [
    "def random_image_list(n):\n",
    "    global validation_loader\n",
    "    image_list = random.sample(list(validation_loader.dataset), n)\n",
    "    images = torch.stack(list(map(lambda x: x[0], image_list)))\n",
    "    labels = torch.tensor(list(map(lambda x: x[1], image_list)))\n",
    "    return images, labels\n",
    "\n",
    "\n",
    "colors = calm_color_generator(80)\n",
    "fig = plt.figure(figsize=(figure_size[0], figure_size[1] * 2),\n",
    "                 constrained_layout=True)\n",
    "\n",
    "gs = gridspec.GridSpec(nrows=4, ncols=1, figure=fig, wspace=None, hspace=.2)\n",
    "\n",
    "bbox_props = dict(boxstyle=\"round\",\n",
    "                  fc=plt.rcParams[\"axes.facecolor\"],\n",
    "                  ec=\"0.5\",\n",
    "                  alpha=0.67)\n",
    "\n",
    "images, labels = random_image_list(20)\n",
    "images_ = images.to(device)\n",
    "labels = labels.to(device)\n",
    "\n",
    "for idx, lr in zip(np.arange(4), lr_list):\n",
    "    model = AlexNet.to(device)\n",
    "    validation_iterations_CNN(epochs=epochs, lr=lr)\n",
    "    outputs = model(images_)\n",
    "    _, preds = torch.max(outputs, 1)\n",
    "    title = \"Learning rate = {}\".format(lr).center(40, \" \")\n",
    "    ax = fig.add_subplot(gs[idx], xticks=[], yticks=[], frameon=False)\n",
    "    ax.set_title(title,\n",
    "                 y=.875,\n",
    "                 loc=\"center\",\n",
    "                 pad=0,\n",
    "                 fontweight=\"heavy\",\n",
    "                 bbox=bbox_props)\n",
    "\n",
    "    for k in range(20):\n",
    "        bgcolor = colors.pop(0)\n",
    "        exec(f\"axins{k} = ax.inset_axes([0.1 * (k % 10), -0.8 * (k // 10), \\\n",
    "        0.1, 0.6])\")\n",
    "        exec(f\"axins{k}.set(xticks=[], yticks=[], frame_on=False)\")\n",
    "        exec(f\"axins{k}.imshow(im_convert(images[k])) if preds[k] == \\\n",
    "        labels[k] else axins{k}.imshow(grayscale_im_convert(images[k]), \\\n",
    "        cmap=\\\"binary\\\")\")\n",
    "\n",
    "        sub_title_1 = f\"{classes[preds[k].item()]}\"\n",
    "        sub_title_2 = f\"({classes[labels[k].item()]})\"\n",
    "        sub_title = sub_title_1 + \"  \" + sub_title_2\n",
    "        if sub_title_1 in [\"ant\"]:\n",
    "            sub_title = sub_title.center(28 - len(sub_title))\n",
    "        else:\n",
    "            sub_title = sub_title.center(27 - len(sub_title))\n",
    "\n",
    "        exec(f\"axins{k}.set_title(sub_title, y=1.025, loc=\\\"center\\\", \\\n",
    "        color=(plt.rcParams[\\\"axes.facecolor\\\"] if preds[k] == labels[k] \\\n",
    "        else bgcolor), backgroundcolor=(bgcolor if preds[k] == labels[k] \\\n",
    "        else plt.rcParams[\\\"axes.facecolor\\\"]), fontweight=\\\"bold\\\")\")\n",
    "\n",
    "fig.suptitle(\n",
    "    \"Visual Comparison of Model Testing Effect under Different Learning Rates\",\n",
    "    fontsize=\"x-large\",\n",
    "    x=0.5,\n",
    "    y=-0.05)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db8dd3c8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-24T12:53:38.954088Z",
     "start_time": "2023-01-24T12:53:38.303760Z"
    },
    "scrolled": false
   },
   "source": [
    "VGG16 = models.vgg16(weights=\"DEFAULT\")\n",
    "\n",
    "tabulation = Form_Generator()\n",
    "tabulation.heading_printer(\"Loading the pre-trained VGG16 model from PyTorch\")\n",
    "\n",
    "statements = [\"\"\"\n",
    "VGG16 = models.vgg16(weights=\"DEFAULT\")\n",
    "\"\"\"]\n",
    "tabulation.statement_generator(statements)\n",
    "\n",
    "variables = [\"VGG16\"]\n",
    "values = [str(VGG16)]\n",
    "tabulation.variable_generator(variables, values)\n",
    "\n",
    "expressions = [\n",
    "    \"len(VGG16.features)\", \"len(VGG16.classifier)\",\n",
    "    \"VGG16.classifier[6].in_features\", \"VGG16.classifier[6].out_features\"\n",
    "]\n",
    "results = [\n",
    "    str(len(VGG16.features)),\n",
    "    str(len(VGG16.classifier)),\n",
    "    str(VGG16.classifier[6].in_features),\n",
    "    str(VGG16.classifier[6].out_features)\n",
    "]\n",
    "tabulation.expression_generator(expressions, results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ee57d77",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-24T12:53:38.960075Z",
     "start_time": "2023-01-24T12:53:38.955094Z"
    },
    "scrolled": false
   },
   "source": [
    "for param in VGG16.features.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "n_inputs = VGG16.classifier[6].in_features\n",
    "last_layer = nn.Linear(n_inputs, len(classes))\n",
    "VGG16.classifier[6] = last_layer\n",
    "\n",
    "tabulation = Form_Generator()\n",
    "tabulation.heading_printer(\n",
    "    \"Modification of pre-trained VGG16 model to fit local datasets\")\n",
    "\n",
    "statements = [\n",
    "    \"\"\"\n",
    "for param in VGG16.features.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "n_inputs = VGG16.classifier[6].in_features\n",
    "last_layer = nn.Linear(n_inputs, len(classes))\n",
    "VGG16.classifier[6] = last_layer\n",
    "\"\"\"\n",
    "]\n",
    "tabulation.statement_generator(statements)\n",
    "\n",
    "variables = [\"n_inputs\", \"last_layer\", \"VGG16\"]\n",
    "values = [str(n_inputs), str(last_layer), str(VGG16)]\n",
    "tabulation.variable_generator(variables, values)\n",
    "\n",
    "expressions = [\n",
    "    \"len(VGG16.features)\", \"len(VGG16.classifier)\",\n",
    "    \"VGG16.classifier[6].in_features\", \"VGG16.classifier[6].out_features\"\n",
    "]\n",
    "results = [\n",
    "    str(len(VGG16.features)),\n",
    "    str(len(VGG16.classifier)),\n",
    "    str(VGG16.classifier[6].in_features),\n",
    "    str(VGG16.classifier[6].out_features)\n",
    "]\n",
    "tabulation.expression_generator(expressions, results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22f98eda",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-24T12:53:39.612131Z",
     "start_time": "2023-01-24T12:53:38.961093Z"
    }
   },
   "source": [
    "model = VGG16.to(device)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.0001)\n",
    "\n",
    "tabulation = Form_Generator()\n",
    "tabulation.heading_printer(\n",
    "    \"Initialization of neural module, criterion and optimizer\")\n",
    "\n",
    "statements = [\n",
    "    \"\"\"\n",
    "model = VGG16.to(device)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.0001)\n",
    "\"\"\"\n",
    "]\n",
    "tabulation.statement_generator(statements)\n",
    "\n",
    "variables = [\"model\", \"criterion\", \"optimizer\"]\n",
    "values = [str(model), str(criterion), str(optimizer)]\n",
    "tabulation.variable_generator(variables, values)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "897fddda",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-24T12:54:20.282138Z",
     "start_time": "2023-01-24T12:53:39.613464Z"
    },
    "scrolled": false
   },
   "source": [
    "running_loss_history = []\n",
    "running_accu_history = []\n",
    "val_running_loss_history = []\n",
    "val_running_accu_history = []\n",
    "\n",
    "dataframe = DataFrame_Generator(\"epoch\", \"loss\", \"accuracy\", \"validation loss\",\n",
    "                                \"validation accuracy\")\n",
    "\n",
    "for e in range(epochs):\n",
    "    running_loss = 0.0\n",
    "    running_accu = 0\n",
    "    val_running_loss = 0.0\n",
    "    val_running_accu = 0\n",
    "\n",
    "    for inputs, labels in training_loader:\n",
    "        inputs = inputs.to(device)\n",
    "        labels = labels.to(device)\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        _, preds = torch.max(outputs, 1)\n",
    "        running_loss += loss.item()\n",
    "        running_accu += torch.sum(preds == labels.data)\n",
    "\n",
    "    else:\n",
    "        with torch.no_grad():\n",
    "            for val_inputs, val_labels in validation_loader:\n",
    "                val_inputs = val_inputs.to(device)\n",
    "                val_labels = val_labels.to(device)\n",
    "                val_outputs = model(val_inputs)\n",
    "                val_loss = criterion(val_outputs, val_labels)\n",
    "\n",
    "                _, val_preds = torch.max(val_outputs, 1)\n",
    "                val_running_loss += val_loss.item()\n",
    "                val_running_accu += torch.sum(val_preds == val_labels.data)\n",
    "\n",
    "        epoch_loss = running_loss / len(training_loader)\n",
    "        epoch_accu = running_accu.float() / len(training_loader.dataset) * 100\n",
    "        running_loss_history.append(epoch_loss)\n",
    "        running_accu_history.append(epoch_accu.item())\n",
    "\n",
    "        val_epoch_loss = val_running_loss / len(validation_loader)\n",
    "        val_epoch_accu = val_running_accu.float() / len(\n",
    "            validation_loader.dataset) * 100\n",
    "        val_running_loss_history.append(val_epoch_loss)\n",
    "        val_running_accu_history.append(val_epoch_accu.item())\n",
    "\n",
    "        dataframe.updater(e + 1, epoch_loss, epoch_accu.item(), val_epoch_loss,\n",
    "                          val_epoch_accu.item())\n",
    "\n",
    "tabulation = Form_Generator()\n",
    "tabulation.heading_printer(\n",
    "    \"Training and testing of pre-trained AlexNet model using local datasets\")\n",
    "\n",
    "statements = [\n",
    "    \"\"\"\n",
    "running_loss_history = []\n",
    "running_accu_history = []\n",
    "val_running_loss_history = []\n",
    "val_running_accu_history = []\n",
    "\n",
    "for e in range(epochs):\n",
    "    running_loss = 0.0\n",
    "    running_accu = 0\n",
    "    val_running_loss = 0.0\n",
    "    val_running_accu = 0\n",
    "\n",
    "    for inputs, labels in training_loader:\n",
    "        inputs = inputs.to(device)\n",
    "        labels = labels.to(device)\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        _, preds = torch.max(outputs, 1)\n",
    "        running_loss += loss.item()\n",
    "        running_accu += torch.sum(preds == labels.data)\n",
    "\n",
    "    else:\n",
    "        with torch.no_grad():\n",
    "            for val_inputs, val_labels in validation_loader:\n",
    "                val_inputs = val_inputs.to(device)\n",
    "                val_labels = val_labels.to(device)\n",
    "                val_outputs = model(val_inputs)\n",
    "                val_loss = criterion(val_outputs, val_labels)\n",
    "\n",
    "                _, val_preds = torch.max(val_outputs, 1)\n",
    "                val_running_loss += val_loss.item()\n",
    "                val_running_accu += torch.sum(val_preds == val_labels.data)\n",
    "\n",
    "        epoch_loss = running_loss / len(training_loader)\n",
    "        epoch_accu = running_accu.float() / len(training_loader.dataset) * 100\n",
    "        running_loss_history.append(epoch_loss)\n",
    "        running_accu_history.append(epoch_accu.item())\n",
    "\n",
    "        val_epoch_loss = val_running_loss / len(validation_loader)\n",
    "        val_epoch_accu = val_running_accu.float() / len(\n",
    "            validation_loader.dataset) * 100\n",
    "        val_running_loss_history.append(val_epoch_loss)\n",
    "        val_running_accu_history.append(val_epoch_accu.item())\n",
    "\"\"\"\n",
    "]\n",
    "tabulation.statement_generator(statements)\n",
    "\n",
    "variables = [\n",
    "    \"epochs\", \"running_loss_history\", \"running_accu_history\",\n",
    "    \"val_running_loss_history\", \"val_running_accu_history\"\n",
    "]\n",
    "values = [\n",
    "    str(epochs),\n",
    "    str(\n",
    "        reprlib_rules.repr(\n",
    "            [np.round(loss, 4) for loss in running_loss_history])),\n",
    "    str(\n",
    "        reprlib_rules.repr(\n",
    "            [np.round(accu, 4) for accu in running_accu_history])),\n",
    "    str(\n",
    "        reprlib_rules.repr(\n",
    "            [np.round(loss, 4) for loss in val_running_loss_history])),\n",
    "    str(\n",
    "        reprlib_rules.repr(\n",
    "            [np.round(accu, 4) for accu in val_running_accu_history]))\n",
    "]\n",
    "tabulation.variable_generator(variables, values, expandtabs=1)\n",
    "\n",
    "df_table = dataframe.tabulation()\n",
    "tabulation.dataframe_generator(df_table)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f3b9f7f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-24T12:54:20.390227Z",
     "start_time": "2023-01-24T12:54:20.283042Z"
    }
   },
   "source": [
    "output_ant = model(img_ant)\n",
    "_ant, pred_ant = torch.max(output_ant, 1)\n",
    "\n",
    "output_bee = model(img_bee)\n",
    "_bee, pred_bee = torch.max(output_bee, 1)\n",
    "\n",
    "tabulation = Form_Generator()\n",
    "tabulation.heading_printer(\n",
    "    \"Test of the classification model with web-scraped image\")\n",
    "\n",
    "statements = [\n",
    "    \"\"\"\n",
    "img_ant = transform(img_ant).to(device).unsqueeze(0)\n",
    "img_bee = transform(img_bee).to(device).unsqueeze(0)\n",
    "\n",
    "output_ant = model(img_ant)\n",
    "_ant, pred_ant = torch.max(output_ant, 1)\n",
    "\n",
    "output_bee = model(img_bee)\n",
    "_bee, pred_bee = torch.max(output_bee, 1)\n",
    "\"\"\"\n",
    "]\n",
    "tabulation.statement_generator(statements)\n",
    "\n",
    "variables = [\n",
    "    \"img_ant\", \"output_ant\", \"_ant\", \"pred_ant\", \"img_bee\", \"output_bee\",\n",
    "    \"_bee\", \"pred_bee\"\n",
    "]\n",
    "values = [\n",
    "    str(reprlib_rules.repr(img_ant.cpu())),\n",
    "    str(output_ant.cpu()),\n",
    "    str(_ant.cpu()),\n",
    "    str(pred_ant.cpu()),\n",
    "    str(reprlib_rules.repr(img_bee.cpu())),\n",
    "    str(output_bee.cpu()),\n",
    "    str(_bee.cpu()),\n",
    "    str(pred_bee.cpu())\n",
    "]\n",
    "tabulation.variable_generator(variables, values)\n",
    "\n",
    "expressions = [\n",
    "    \"img_ant.shape\", \"torch.max(output_ant, 0)\", \"torch.max(output_ant, 1)\",\n",
    "    \"pred_ant.item()\", \"classes[pred_ant.item()]\", \"img_bee.shape\",\n",
    "    \"torch.max(output_bee, 0)\", \"torch.max(output_bee, 1)\", \"pred_bee.item()\",\n",
    "    \"classes[pred_bee.item()]\"\n",
    "]\n",
    "results = [\n",
    "    str(img_ant.shape),\n",
    "    str(torch.max(output_ant.cpu(), 0)),\n",
    "    str(torch.max(output_ant.cpu(), 1)),\n",
    "    str(pred_ant.item()), classes[pred_ant.item()],\n",
    "    str(img_bee.shape),\n",
    "    str(torch.max(output_bee.cpu(), 0)),\n",
    "    str(torch.max(output_bee.cpu(), 1)),\n",
    "    str(pred_bee.item()), classes[pred_bee.item()]\n",
    "]\n",
    "tabulation.expression_generator(expressions, results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bd4e278",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-24T12:56:20.034799Z",
     "start_time": "2023-01-24T12:54:20.395351Z"
    }
   },
   "source": [
    "colors = calm_color_generator(16)\n",
    "plt.rcParams[\"figure.figsize\"] = (figure_size[0], figure_size[1])\n",
    "\n",
    "fig, axs = plt.subplots(2, 2)\n",
    "\n",
    "epochs_list = [10, 5]\n",
    "\n",
    "for i in range(2):\n",
    "    for j in range(2):\n",
    "        epochs = epochs_list[j]\n",
    "        model = VGG16.to(device)\n",
    "        loss, accu, val_loss, val_accu = validation_iterations_CNN(\n",
    "            epochs=epochs, lr=lr_list[2 + i])\n",
    "        line1 = axs[i, j].plot(range(1, epochs + 1),\n",
    "                               loss,\n",
    "                               label=\"training loss\",\n",
    "                               linestyle=\":\",\n",
    "                               c=colors.pop(0))\n",
    "        line2 = axs[i, j].plot(range(1, epochs + 1),\n",
    "                               val_loss,\n",
    "                               label=\"validation loss\",\n",
    "                               linestyle=\"-.\",\n",
    "                               c=colors.pop(0))\n",
    "        axs[i, j].set(xlabel=\"epoch\", ylabel=\"loss\")\n",
    "        axs[i, j] = axs[i, j].twinx()\n",
    "        line3 = axs[i, j].plot(range(1, epochs + 1),\n",
    "                               accu,\n",
    "                               label=\"training accuracy\",\n",
    "                               linestyle=\"--\",\n",
    "                               c=colors.pop(0))\n",
    "        line4 = axs[i, j].plot(range(1, epochs + 1),\n",
    "                               val_accu,\n",
    "                               label=\"validation accuracy\",\n",
    "                               c=colors.pop(0))\n",
    "        axs[i, j].set(ylabel=\"accuracy (%)\")\n",
    "        lines = line1 + line2 + line3 + line4\n",
    "        labels = [line.get_label() for line in lines]\n",
    "        axs[i, j].legend(lines, labels, loc=\"lower left\", borderpad=1, ncol=2)\n",
    "\n",
    "epochs_list_ = epochs_list * 2\n",
    "lr_list_ = list(chain.from_iterable(zip(*[lr_list[2:]] * 2)))\n",
    "\n",
    "for ax, epochs, lr in zip(axs.flat, epochs_list_, lr_list_):\n",
    "    ax.set_title(\"Number of epochs = {}, learning rate = {}\".format(\n",
    "        epochs, lr),\n",
    "                 loc=\"center\",\n",
    "                 pad=15)\n",
    "\n",
    "fig.suptitle(\n",
    "    \"Visual Comparison of Model Testing with Different Learning Rates \"\n",
    "    \"and / or Different Number of Epochs\",\n",
    "    fontsize=\"x-large\",\n",
    "    x=0.5,\n",
    "    y=0)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24d920f0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-24T12:56:20.039490Z",
     "start_time": "2023-01-24T12:56:20.036559Z"
    }
   },
   "source": [
    "epochs = 5\n",
    "\n",
    "tabulation = Form_Generator()\n",
    "tabulation.heading_printer(\"Reducing training epochs to avoid overfitting\")\n",
    "\n",
    "statements = [\"\"\"\n",
    "epochs = 5\n",
    "\"\"\"]\n",
    "tabulation.statement_generator(statements)\n",
    "\n",
    "variables = [\"epochs\"]\n",
    "values = [str(epochs)]\n",
    "tabulation.variable_generator(variables, values)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5549d7eb",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-24T12:57:38.600447Z",
     "start_time": "2023-01-24T12:56:20.040461Z"
    }
   },
   "source": [
    "colors = calm_color_generator(16)\n",
    "plt.rcParams[\"figure.figsize\"] = (figure_size[0], figure_size[1])\n",
    "\n",
    "fig, axs = plt.subplots(2, 2, sharex=True)\n",
    "\n",
    "for i in range(2):\n",
    "    for j in range(2):\n",
    "        model = VGG16.to(device)\n",
    "        loss, accu, val_loss, val_accu = validation_iterations_CNN(\n",
    "            epochs=epochs, lr=lr_list[i * 2 + j])\n",
    "        line1 = axs[i, j].plot(range(1, epochs + 1),\n",
    "                               loss,\n",
    "                               label=\"training loss\",\n",
    "                               linestyle=\":\",\n",
    "                               c=colors.pop(0))\n",
    "        line2 = axs[i, j].plot(range(1, epochs + 1),\n",
    "                               val_loss,\n",
    "                               label=\"validation loss\",\n",
    "                               linestyle=\"-.\",\n",
    "                               c=colors.pop(0))\n",
    "        axs[i, j].set(xlabel=\"epoch\", ylabel=\"loss\")\n",
    "        axs[i, j] = axs[i, j].twinx()\n",
    "        line3 = axs[i, j].plot(range(1, epochs + 1),\n",
    "                               accu,\n",
    "                               label=\"training accuracy\",\n",
    "                               linestyle=\"--\",\n",
    "                               c=colors.pop(0))\n",
    "        line4 = axs[i, j].plot(range(1, epochs + 1),\n",
    "                               val_accu,\n",
    "                               label=\"validation accuracy\",\n",
    "                               c=colors.pop(0))\n",
    "        axs[i, j].set(ylabel=\"accuracy (%)\")\n",
    "        lines = line1 + line2 + line3 + line4\n",
    "        labels = [line.get_label() for line in lines]\n",
    "        axs[i, j].legend(lines, labels, loc=\"lower left\", borderpad=1, ncol=2)\n",
    "\n",
    "for ax, lr in zip(axs.flat, lr_list):\n",
    "    ax.set_title(\"Learning rate = {}\".format(lr), loc=\"center\", pad=15)\n",
    "\n",
    "fig.suptitle(\n",
    "    \"Visual Comparison of Model Testing with Different Learning Rates\",\n",
    "    fontsize=\"x-large\",\n",
    "    x=0.5,\n",
    "    y=0)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f82cb6c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-24T12:58:26.349396Z",
     "start_time": "2023-01-24T12:57:38.601658Z"
    }
   },
   "source": [
    "colors = calm_color_generator(80)\n",
    "fig = plt.figure(figsize=(figure_size[0], figure_size[1] * 2),\n",
    "                 constrained_layout=True)\n",
    "\n",
    "gs = gridspec.GridSpec(nrows=4, ncols=1, figure=fig, wspace=None, hspace=.2)\n",
    "\n",
    "bbox_props = dict(boxstyle=\"round\",\n",
    "                  fc=plt.rcParams[\"axes.facecolor\"],\n",
    "                  ec=\"0.5\",\n",
    "                  alpha=0.67)\n",
    "\n",
    "images, labels = random_image_list(20)\n",
    "images_ = images.to(device)\n",
    "labels = labels.to(device)\n",
    "\n",
    "for idx, lr in zip(np.arange(4), lr_list):\n",
    "    model = AlexNet.to(device)\n",
    "    validation_iterations_CNN(epochs=epochs, lr=lr)\n",
    "    outputs = model(images_)\n",
    "    _, preds = torch.max(outputs, 1)\n",
    "    title = \"Learning rate = {}\".format(lr).center(40, \" \")\n",
    "    ax = fig.add_subplot(gs[idx], xticks=[], yticks=[], frameon=False)\n",
    "    ax.set_title(title,\n",
    "                 y=.875,\n",
    "                 loc=\"center\",\n",
    "                 pad=0,\n",
    "                 fontweight=\"heavy\",\n",
    "                 bbox=bbox_props)\n",
    "\n",
    "    for k in range(20):\n",
    "        bgcolor = colors.pop(0)\n",
    "        exec(f\"axins{k} = ax.inset_axes([0.1 * (k % 10), -0.8 * (k // 10), \\\n",
    "        0.1, 0.6])\")\n",
    "        exec(f\"axins{k}.set(xticks=[], yticks=[], frame_on=False)\")\n",
    "        exec(f\"axins{k}.imshow(im_convert(images[k])) if preds[k] == \\\n",
    "        labels[k] else axins{k}.imshow(grayscale_im_convert(images[k]), \\\n",
    "        cmap=\\\"binary\\\")\")\n",
    "\n",
    "        sub_title_1 = f\"{classes[preds[k].item()]}\"\n",
    "        sub_title_2 = f\"({classes[labels[k].item()]})\"\n",
    "        sub_title = sub_title_1 + \"  \" + sub_title_2\n",
    "        if sub_title_1 in [\"ant\"]:\n",
    "            sub_title = sub_title.center(28 - len(sub_title))\n",
    "        else:\n",
    "            sub_title = sub_title.center(27 - len(sub_title))\n",
    "\n",
    "        exec(f\"axins{k}.set_title(sub_title, y=1.025, loc=\\\"center\\\", \\\n",
    "        color=(plt.rcParams[\\\"axes.facecolor\\\"] if preds[k] == labels[k] \\\n",
    "        else bgcolor), backgroundcolor=(bgcolor if preds[k] == labels[k] \\\n",
    "        else plt.rcParams[\\\"axes.facecolor\\\"]), fontweight=\\\"bold\\\")\")\n",
    "\n",
    "fig.suptitle(\n",
    "    \"Visual Comparison of Model Testing Effect under Different Learning Rates\",\n",
    "    fontsize=\"x-large\",\n",
    "    x=0.5,\n",
    "    y=-0.05)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9c25643",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-24T12:59:12.533193Z",
     "start_time": "2023-01-24T12:58:26.350532Z"
    }
   },
   "source": [
    "colors = calm_color_generator(100)\n",
    "fig = plt.figure(figsize=(figure_size[0], figure_size[1] * 2),\n",
    "                 constrained_layout=True)\n",
    "\n",
    "gs = gridspec.GridSpec(nrows=10, ncols=2, figure=fig, wspace=.05, hspace=None)\n",
    "\n",
    "bbox_props = dict(boxstyle=\"round\",\n",
    "                  fc=plt.rcParams[\"axes.facecolor\"],\n",
    "                  ec=\"0.5\",\n",
    "                  alpha=0.67)\n",
    "\n",
    "images, labels = random_image_list(50)\n",
    "images_ = images.to(device)\n",
    "labels = labels.to(device)\n",
    "\n",
    "axis_titles = [\"AlexNet Architecture\", \"VGG16 Architecture\"]\n",
    "\n",
    "for idx, title in zip(np.arange(2), axis_titles):\n",
    "    if idx == 0:\n",
    "        epochs = 10\n",
    "        model = AlexNet.to(device)\n",
    "        validation_iterations_CNN(epochs=epochs, lr=lr_list[3])\n",
    "    else:\n",
    "        epochs = 5\n",
    "        model = VGG16.to(device)\n",
    "        validation_iterations_CNN(epochs=epochs, lr=lr_list[3])\n",
    "\n",
    "    outputs = model(images_)\n",
    "    _, preds = torch.max(outputs, 1)\n",
    "    title = title.center(40, \" \")\n",
    "    ax = fig.add_subplot(gs[idx], xticks=[], yticks=[], frameon=False)\n",
    "    ax.set_title(title,\n",
    "                 y=1.6,\n",
    "                 loc=\"center\",\n",
    "                 pad=0,\n",
    "                 fontweight=\"heavy\",\n",
    "                 bbox=bbox_props)\n",
    "\n",
    "    for k in range(50):\n",
    "        bgcolor = colors.pop(0)\n",
    "        exec(f\"axins{k} = ax.inset_axes([0.2 * (k % 5), -1.4 * (k // 5), \\\n",
    "        0.189, 1.3])\")\n",
    "        exec(f\"axins{k}.set(xticks=[], yticks=[], frame_on=False)\")\n",
    "        exec(f\"axins{k}.imshow(im_convert(images[k])) if preds[k] == \\\n",
    "        labels[k] else axins{k}.imshow(grayscale_im_convert(images[k]), \\\n",
    "        cmap=\\\"binary\\\")\")\n",
    "\n",
    "        sub_title_1 = f\"{classes[preds[k].item()]}\"\n",
    "        sub_title_2 = f\"({classes[labels[k].item()]})\"\n",
    "        sub_title = sub_title_1 + \"  \" + sub_title_2\n",
    "        if sub_title_1 in [\"ant\"]:\n",
    "            sub_title = sub_title.center(28 - len(sub_title))\n",
    "        else:\n",
    "            sub_title = sub_title.center(27 - len(sub_title))\n",
    "\n",
    "        exec(f\"axins{k}.set_title(sub_title, y=1.025, loc=\\\"center\\\", \\\n",
    "        color=(plt.rcParams[\\\"axes.facecolor\\\"] if preds[k] == labels[k] \\\n",
    "        else bgcolor), backgroundcolor=(bgcolor if preds[k] == labels[k] \\\n",
    "        else plt.rcParams[\\\"axes.facecolor\\\"]), fontweight=\\\"bold\\\")\")\n",
    "\n",
    "fig.suptitle(\n",
    "    \"Visual Comparison of Model Testing Effect under Different \"\n",
    "    \"Convolutional Neural Networks Architectures\",\n",
    "    fontsize=\"x-large\",\n",
    "    x=0.5,\n",
    "    y=-0.125)\n",
    "\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  },
  "nbTranslate": {
   "displayLangs": [
    "*"
   ],
   "hotkey": "alt-t",
   "langInMainMenu": true,
   "sourceLang": "en",
   "targetLang": "fr",
   "useGoogleTranslate": true
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "345.594px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
